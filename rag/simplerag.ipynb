{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data Loading Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseLoader.load of <langchain_community.document_loaders.text.TextLoader object at 0x0000013431A300D0>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No.1 TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_doc=loader.load;\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTop 100+ Data Science Interview Questions and Answers (2024)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA to DevelopmentFor Working ProfessionalsData Structure & Algorithm Classes (Live)System Design (Live)JAVA Backend Development(Live)DevOps(Live)Data Structures & Algorithms in PythonFor StudentsInterview Preparation CourseGATE CS & IT 2024Data Science (Live)Data Structure & Algorithm-Self Paced(C++/JAVA)Master Competitive Programming(Live)Full Stack Development with React & Node JS(Live)For School StudentsCBSE Class 12 Computer ScienceSchool GuidePython Programming FoundationAll CoursesTutorialsPython TutorialTaking Input in PythonPython OperatorsPython VariablesPython Data TypesData TypesPython NumbersPython StringPython ListsPython TuplesSets in PythonPython DictionaryPython Loops and Control FlowConditional StatementsLoops in PythonPython If ElsePython For LoopsPython While LoopsPython BreaksPython Continue StatementPython Pass StatementPython FunctionsPython OOPS ConceptDSA in PythonPython Exception HandlingPython File HandlingComplete Python TutorialPython ExercisesPython List ExercisePython String ExercisePython Tuple ExercisePython Dictionary ExercisePython Set ExercisePython Design PatternsPython Programming ExamplesPython Practice QuestionsWeb Development in PythonDjangoDjango TutorialTop Django ProjectsDjango Interview QuestionsFlaskFlask TutorialPython Flask ProjectsFlask Interview QuestionsPostmanGithubWeb Scrapping in PythonData Structures & AlgorithmsDSA for BeginnersData StructuresArraysMatrixStringsLinked ListStackQueueTreeGeneric TreeBinary TreeBinary Search TreeAVL TreeB TreeB+ TreeRed Black TreeTree Data Structure TutorialHeapHashingGraphSet Data StructureMap Data StructureAdvanced Data StructureData Structures TutorialAlgorithmsAnalysis of AlgorithmsSearching AlgorithmsLinear SearchBinary SearchSearching Algorithms TutorialSorting AlgorithmsSelection SortBubble SortInsertion SortMerge SortQuick SortHeap SortCounting SortRadix SortBucket SortSorting Algorithms TutorialGreedy AlgorithmsDynamic ProgrammingGraph AlgorithmsPattern SearchingRecursionBacktrackingDivide and ConquerMathematical AlgorithmsGeometric AlgorithmsBitwise AlgorithmsRandomized AlgorithmsBranch and BoundAlgorithms TutorialComplete DSA TutorialCompetitive ProgrammingCompany Wise SDE SheetsFacebook SDE SheetAmazon SDE SheetApple SDE SheetNetflix SDE SheetGoogle SDE SheetWipro Coding SheetInfosys Coding SheetTCS Coding SheetCognizant Coding SheetHCL Coding SheetDSA Cheat SheetsDSA Sheet for BeginnersSDE SheetsFAANG Coding SheetLove Babbar SheetMass Recruiter SheetProduct-Based Coding SheetCompany-Wise Preparation SheetTop 100 DSA Interview Questions Topic-wise100 Days of CodeDSA BlogsSystem DesignSystem Design TutorialSoftware Design PatternsSystem Design RoadmapTop 10 System Design Interview QuestionsInterview CornerCompany PreparationTop TopicsPractice Company QuestionsInterview ExperiencesExperienced InterviewsInternship InterviewsCompetitive ProgrammingMultiple Choice QuizzesAptitude for PlacementsPuzzles for InterviewsLanguagesCC++JavaPythonR TutorialC#SQLScalaPerlGo LanguageWeb DevelopmentHTMLCSSJavaScriptTypeScriptReactJSReact JS TutorialNextJSNode.jsPHPAngularJSjQueryWeb Design100 Days of Web DevelopmentCS SubjectsEngineering MathematicsOperating SystemDBMSComputer NetworksComputer Organization and ArchitectureTheory of ComputationCompiler DesignDigital LogicSoftware EngineeringDevOpsDevOps TutorialGITAWSKubernetesDockerMicrosoft Azure TutorialGoogle Cloud PlatformDevOps RoadmapDevOps Interview QuestionsLinuxLinux TutorialLinux Commands A-ZLinux Commands CheatsheetFile Permissions in LinuxLinux System AdministrationLinux File SystemLinux Shell ScriptingLinux NetworkingLinux Interview QuestionsSchool LearningClass 8 Study MaterialClass 9 Study MaterialClass 10 Study MaterialClass 11 Study MaterialClass 12 Study MaterialEnglish GrammarGfG SchoolCommerceBusiness StudiesClass 11th NotesClass 12th NotesBusiness Studies Complete GuideAccountancyClass 11th NotesClass 12th NotesAccountancy Complete GuideMicroeconomicsClass 11th NotesMicroeconomics Complete GuideStatistics for EconomicsClass 11th NotesStatistics for Economics Complete GuideMacroeconomicsClass 12th notesMacroeconomics Complete GuideIndian Economic DevelopmentClass 12th NotesIndian Economic Development Complete GuideHuman Resource Management (HRM)ManagementIncome TaxFinanceCommerce Complete GuideGATEGATE Computer Science NotesLast Minute NotesGATE CS Solved PapersGATE CS Original Papers and Official KeysGATE CS 2025 SyllabusGATE DA 2025 SyllabusOther CS ExamsISROUGC NETUPSC and SSC/BankingUPSC Study MaterialBanking Exams Study MaterialSBI ClerkSBI POIBPS ClerkIBPS POSSC CGL Study MaterialGeeksforGeeks VideosData SciencePython TutorialR TutorialMachine LearningML TutorialML ProjectsML Maths100 Days of Machine LearningData Science using PythonData Science using RData Science PackagesPandas TutorialNumPy TutorialData VisualizationPython Data Visualization with PythonPython Data Visualization TutorialMatplotlib TutorialBokeh TutorialPlotly TutorialSeaborn TutorialData Visualization with RData Visualization with RPlotly Tutorialggplot TutorialData Visualization ToolsTableauPower BIData AnalysisData Analysis with PythonData Analysis with R100 Days of Data AnalyticsDeep LearningDeep Learning TutorialDeep Learning ProjectsNLP TutorialComputer VisionComputer Vision TutorialComputer Vision ProjectsInterview CornerMachine Learning Interview QuestionDeep Learning Interview QuestionNLP Interview QuestionPython Interview QuestionsTop 50 R Interview QuestionsML FrameworksTensorflowPyTorchPyTorch LightningPracticeAll DSA ProblemsProblem of the DayGFG SDE SheetCurated DSA ListsBeginner's DSA SheetLove Babbar SheetTop 50 Array ProblemsTop 50 String ProblemsTop 50 Tree ProblemsTop 50 Graph ProblemsTop 50 DP Problems\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPythonR LanguagePython for Data ScienceNumPyPandasOpenCVData AnalysisML MathMachine LearningNLPDeep LearningDeep Learning Interview QuestionsMachine LearningML ProjectsML Interview Questions \\n\\n\\n\\n\\n▲\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDouble Savings Offer on CoursesWrite AI ML & Data Science ArticlesData Science Interview Questions and AnswersWhat is Data Science?Basic Data Science Interview Questions For FresherQ.1 What is marginal probability?Q.2 What are the probability axioms?Q.3 What is conditional probability?Q.4 What is Bayes Theorem and when is it used in data science?Q.5 Define variance and conditional variance.Q.6 Explain the concepts of mean, median, mode, and standard deviation.Q.7 What is the normal distribution and standard normal distribution?Q.8 What is SQL, and what does it stand for?Q.9 Explain the differences between SQL and NoSQL databases.Q.10 What are the primary SQL database management systems (DBMS)?Q.11 What is the ER model in SQL?Q.12 What is data transformation?Q.13 What are the main components of a SQL query?Q.14 What is a primary key?Q.15 What is the purpose of the GROUP BY clause, and how is it used?Q.16 What is the WHERE clause used for, and how is it used to filter data?Q.17 How do you retrieve distinct values from a column in SQL?Q.18 What is the HAVING clause?Q.19 How do you handle missing or NULL values in a database table?Q.20 What is the difference between supervised and unsupervised machine learning?Q.21 What is linear regression, and What are the different assumptions of linear regression algorithms?Q.22 Logistic regression is a classification technique, why its name is regressions, not logistic classifications?Q.23 What is the logistic function (sigmoid function) in logistic regression?Q.24 What is overfitting and how can be overcome this?Q.25 What is a support vector machine (SVM), and what are its key components?Q.26 Explain the k-nearest neighbors (KNN) algorithm.Q.27 What is the Naive Bayes algorithm, what are the different assumptions of Naive Bayes?Q.28 What are decision trees, and how do they work?Q.29 Explain the concepts of entropy and information gain in decision trees.Q.30 What is the difference between the bagging and boosting model?Q.31 Describe random forests and their advantages over single-decision trees.Q.32 What is K-Means, and how will it work?Q.33 What is a confusion matrix? Explain with an example.Q.34 What is a classification report and explain the parameters used to interpret the result of classification tasks with an example.Intermediate Data Science Interview QuestionsQ.35 Explain the uniform distribution.Q.36 Describe the Bernoulli distribution.Q.37 What is the binomial distribution?Q.38 Explain the exponential distribution and where it's commonly used.Q.39 Describe the Poisson distribution and its characteristics.Q40. Explain the t-distribution and its relationship with the normal distribution.Q.41 Describe the chi-squared distribution.Q.42 What is the difference between z-test, F-test, and t-test?Q.43 What is the central limit theorem, and why is it significant in statistics?Q.44 Describe the process of hypothesis testing, including null and alternative hypotheses.Q.45 How do you calculate a confidence interval, and what does it represent?Q.46 What is a p-value in Statistics?Q.47 Explain Type I and Type II errors in hypothesis testing.Q.48 What is the significance level (alpha) in hypothesis testing?Q.49 How can you calculate the correlation coefficient between two variables?Q.50 What is covariance, and how is it related to correlation?Q.51 Explain how to perform a hypothesis test for comparing two population means.Q.52 Explain the concept of normalization in database design.Q.53 What is database normalization?Q.54 Define different types of SQL functions.Q.55 Explain the difference between INNER JOIN and LEFT JOIN.Q.56 What is a subquery, and how can it be used in SQL?Q.57 How do you perform mathematical calculations in SQL queries?Q.58 What is the purpose of the CASE statement in SQL?Q.59 What is the difference between a database and a data warehouse?Q.60 What is regularization in machine learning, State the differences between L1 and L2 regularizationQ.61 Explain the concepts of bias-variance trade-off in machine learning.Q.62 How do we choose the appropriate kernel function in SVM?Q.63 How does Naive Bayes handle categorical and continuous features?Q.64 What is Laplace smoothing (add-one smoothing) and why is it used in Naive Bayes?Q.65 What are imbalanced datasets and how can we handle them?Q.66 What are outliers in the dataset and how can we detect and remove them?Q.67 What is the curse of dimensionality And How can we overcome this?Q.68 How does the random forest algorithm handle feature selection?Q.69 What is feature engineering? Explain the different feature engineering methods.Q.70 How will we deal with the categorical text values in machine learning?Q.71 What is DBSCAN and How will we use it?Q.72 How does the EM (Expectation-Maximization) algorithm work in clustering?Q.73 Explain the concept of silhouette score in clustering evaluation.Q.74 What is the relationship between eigenvalues and eigenvectors in PCA?Q.75 What is the cross-validation technique in machine learning?Q.76 What are the ROC and AUC, explain its significance in binary classification.Q.77 Describe gradient descent and its role in optimizing machine learning modelsQ.78 Describe batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.Q.79 Explain the Apriori - Association Rule MiningData Science Interview Questions for ExperiencedQ.80 Explain multivariate distribution in data science.Q.81 Describe the concept of conditional probability density function (PDF).Q.82 What is the cumulative distribution function (CDF), and how is it related to PDF?Q.83 What is ANOVA? What are the different ways to perform ANOVA tests?Q.84 How can you prevent gradient descent from getting stuck in local minima?Q.85 Explain the Gradient Boosting algorithms in machine learning.Q.86 Explain convolutions operations of CNN architecture?Q.87 What is feed forward network and how it is different from recurrent neural network?Q.88 Explain the difference between generative and discriminative models?Q.89 What is the forward and backward propagations in deep learning?Q.90 Describe the use of Markov models in sequential data analysis?Q.91 What is generative AI?Q.92 What are different neural network architectures used to generate artificial data in deep learning?Q.93 What is deep reinforcement learning technique?Q.94 What is transfer learning, and how is it applied in deep learning?Q.95 What is the difference between object detection and image segmentation.Q.96 Explain the concept of word embeddings in natural language processing (NLP).Q.97 What is seq2seq model?Q.98 What is artificial neural networks.Q.99 What is marginal probability?Q.100 What are the probability axioms?Machine Learning & Data Science Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nData Science Interview Questions and Answers\\n\\n\\nLast Updated : \\n19 Mar, 2024\\n\\n\\n\\n \\n\\nImprove\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n \\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\nData Science Interview Questions – Explore the Data Science Interview Questions and Answers for beginners and experienced professionals looking for new opportunities in data science. \\n\\nWe all know that data science is a field where data scientists mine raw data, analyze it, and extract useful insights from it. The article outlines the frequently asked questionas during the data science interview. Practising all the below questions will help you to explore your career as a data scientist.\\nTable of Content\\nBasic Data Science Interview Questions For FresherIntermediate Data Science Interview Questions Data Science Interview Questions for ExperiencedWhat is Data Science?Data science is a field that extracts knowledge and insights from structured and unstructured data by using scientific methods, algorithms, processes, and systems. It combines expertise from various domains, such as statistics, computer science, machine learning, data engineering, and domain-specific knowledge, to analyze and interpret complex data sets.\\nFurthermore, data scientists use a combination of multiple languages, such as Python and R. They are also frequent users of data analysis tools like pandas, NumPy, and scikit-learn, as well as machine learning libraries.\\nAfter exploring the brief of data science, let’s dig into the data science interview questions and answers.\\nBasic Data Science Interview Questions For FresherQ.1 What is marginal probability?A key idea in statistics and probability theory is marginal probability, which is also known as marginal distribution. With reference to a certain variable of interest, it is the likelihood that an event will occur, without taking into account the results of other variables. Basically, it treats the other variables as if they were “marginal” or irrelevant and concentrates on one.\\nMarginal probabilities are essential in many statistical analyses, including estimating anticipated values, computing conditional probabilities, and drawing conclusions about certain variables of interest while taking other variables’ influences into account.\\nQ.2 What are the probability axioms?The fundamental rules that control the behaviour and characteristics of probabilities in probability theory and statistics are referred to as the probability axioms, sometimes known as the probability laws or probability principles.\\nThere are three fundamental axioms of probability:\\nNon-Negativity AxiomNormalization AxiomAdditivity AxiomQ.3 What is conditional probability?The event or outcome occurring based on the existence of a prior event or outcome is known as conditional probability. It is determined by multiplying the probability of the earlier occurrence by the increased lprobability of the later, or conditional, event.\\nQ.4 What is Bayes’ Theorem and when is it used in data science?The Bayes theorem predicts the probability that an event connected to any condition would occur. It is also taken into account in the situation of conditional probability. The probability of “causes” formula is another name for the Bayes theorem.\\nIn data science, Bayes’ Theorem is used primarily in:\\nBayesian InferenceMachine LearningText ClassificationMedical DiagnosisPredictive ModelingWhen working with ambiguous or sparse data, Bayes’ Theorem is very helpful since it enables data scientists to continually revise their assumptions and come to more sensible conclusions.\\nQ.5 Define variance and conditional variance.A statistical concept known as variance quantifies the spread or dispersion of a group of data points within a dataset. It sheds light on how widely individual data points depart from the dataset’s mean (average). It assesses the variability or “scatter” of data.\\nConditional Variance\\nA measure of the dispersion or variability of a random variable under certain circumstances or in the presence of a particular event, as the name implies. It reflects a random variable’s variance that is dependent on the knowledge of another random variable’s variance.\\nQ.6 Explain the concepts of mean, median, mode, and standard deviation.Mean:\\xa0The mean, often referred to as the average, is calculated by summing up all the values in a dataset and then dividing by the total number of values.\\nMedian:\\xa0When data are sorted in either ascending or descending order, the median is the value in the middle of the dataset. The median is the average of the two middle values when the number of data points is even.In comparison to the mean, the median is less impacted by extreme numbers, making it a more reliable indicator of central tendency.\\nMode:\\xa0The value that appears most frequently in a dataset is the mode. One mode (unimodal), several modes (multimodal), or no mode (if all values occur with the same frequency) can all exist in a dataset.\\nStandard deviation: The spread or dispersion of data points in a dataset is measured by the standard deviation. It quantifies the variance between different data points.\\nQ.7 What is the normal distribution and standard normal distribution?The normal distribution, also known as the Gaussian distribution or bell curve, is a continuous probability distribution that is characterized by its symmetric bell-shaped curve. The normal distribution is defined by two parameters: the mean (μ) and the standard deviation (σ). The mean determines the center of the distribution, and the standard deviation determines the spread or dispersion of the distribution. The distribution is symmetric around its mean, and the bell curve is centered at the mean. The probabilities for values that are further from the mean taper off equally in both directions. Similar rarity applies to extreme values in the two tails of the distribution. Not all symmetrical distributions are normal, even though the normal distribution is symmetrical.\\nThe standard normal distribution, also known as the Z distribution, is a special case of the normal distribution where the mean (μ) is 0 and the standard deviation (σ) is 1. It is a standardized form of the normal distribution, allowing for easy comparison of scores or observations from different normal distributions.\\nQ.8\\xa0What is SQL,\\xa0and what does it stand for?SQL stands for Structured Query Language.It is a specialized programming language used for managing and manipulating relational databases. It is designed for tasks related to database management, data retrieval, data manipulation, and data definition.\\nQ.9 Explain the differences between SQL and NoSQL databases.Both\\xa0SQL\\xa0(Structured Query Language) and NoSQL (Not Only SQL) databases, differ in their data structures, schema, query languages, and use cases. The following are the main variations between SQL and NoSQL databases.\\nSQL\\nNoSQL\\nSQL databases are relational databases, they organise and store data using a structured schema with tables, rows, and columns.\\nNoSQL databases use a number of different types of data models, such as document-based (like JSON and BSON), key-value pairs, column families, and graphs.\\nSQL databases have a set schema, thus before inserting data, we must establish the structure of our data.The schema may need to be changed, which might be a difficult process.\\nNoSQL databases frequently employ a dynamic or schema-less approach, enabling you to insert data without first creating a predetermined schema.\\nSQL is a strong and standardised query language that is used by SQL databases. Joins, aggregations, and subqueries are only a few of the complicated processes supported by SQL queries.\\nThe query languages or APIs used by NoSQL databases are frequently tailored to the data model.\\nQ.10 What are the primary SQL database management systems (DBMS)?Relational database systems, both open source and commercial, are the main SQL (Structured Query Language) database management systems (DBMS), which are widely used for managing and processing structured data. Some of the most popular SQL database management systems are listed below:\\nMySQLMicrosoft SQL ServerSQLitePostgreSQLOracle DatabaseAmazon RDSQ.11 What is the ER model in SQL?The structure and relationships between the data entities in a database are represented by the Entity-Relationship (ER) model, a conceptual framework used in database architecture. The ER model is frequently used in conjunction with SQL for creating the structure of relational databases even though it is not a component of the SQL language itself.\\nQ.12 What is data transformation?The process of transforming data from one structure, format, or representation into another is referred to as data transformation. In order to make the data more suited for a given goal, such as analysis, visualisation, reporting, or storage, this procedure may involve a variety of actions and changes to the data. Data integration, cleansing, and analysis depend heavily on data transformation, which is a common stage in data preparation and processing pipelines.\\nQ.13 What are the main components of a SQL query?A relational database’s data can be retrieved, modified, or managed via a SQL (Structured Query Language) query. The operation of a SQL query is defined by a number of essential components, each of which serves a different function.\\nSELECTFROMWHEREGROUP BYHAVINGORDER BYLIMITJOINQ.14 What is a primary key?A relational database table’s main key, also known as a primary keyword, is a column that is unique for each record. It is a distinctive identifier.The primary key of a relational database must be unique. Every row of data must have a primary key value and none of the rows can be null.\\nQ.15 What is the purpose of the GROUP BY clause, and how is it used?In SQL, the GROUP BY clause is used to create summary rows out of rows that have the same values in a set of specified columns. In order to do computations on groups of rows as opposed to individual rows, it is frequently used in conjunction with aggregate functions like SUM, COUNT, AVG, MAX, or MIN. we may produce summary reports and perform more in-depth data analysis using the GROUP BY clause.\\nQ.16 What is the WHERE clause used for, and how is it used to filter data?In SQL, the WHERE clause is used to filter rows from a table or result set according to predetermined criteria. It enables us to pick only the rows that satisfy particular requirements or follow a pattern. A key element of SQL queries, the WHERE clause is frequently used for data retrieval and manipulation.\\nQ.17 How do you retrieve distinct values from a column in SQL?Using the DISTINCT keyword in combination with the SELECT command, we can extract distinct values from a column in SQL. By filtering out duplicate values and returning only unique values from the specified column, the DISTINCT keyword is used.\\nQ.18 What is the HAVING clause?To filter query results depending on the output of aggregation functions, the HAVING clause, a SQL clause, is used along with the GROUP BY clause. The HAVING clause filters groups of rows after they have been grouped by one or more columns, in contrast to the WHERE clause, which filters rows before they are grouped.\\nQ.19 How do you handle missing or NULL values in a database table?Missing or NULL values can arise due to various reasons, such as incomplete data entry, optional fields, or data extraction processes.\\nReplace NULL with Placeholder ValuesHandle NULL Values in QueriesUse Default ValuesQ.20 What is the difference between supervised and unsupervised machine learning?The difference between Supervised Learning and Unsupervised Learning are as follow:\\nCategory\\nSupervised Learning\\nUnsupervised Learning\\nDefinition\\nSupervised learning refers to that part of machine learning where we know what the target variable is and it is labeled.\\nUnsupervised Learning is used when we do not have labeled data and we are not sure about our target variable\\nObjective\\nThe objective of supervised learning is to predict an outcome or classify the data\\nThe objective here is to discover patterns among the features of the dataset and group similar features together\\nAlgorithms\\nSome of the algorithm types are:\\nRegression (Linear, Logistic, etc.)Classification (Decision Tree Classifier, Support Vector Classifier, etc.)Some of the algorithms are :\\nDimensionality reduction (Principle Component Analysis, etc.)Clustering (KMeans, DBSCAN, etc.)Evaluation metrics\\nSupervised learning uses evaluation metrics like:\\nMean Squared ErrorAccuracyUnsupervised Learning uses evaluation metrics like:\\nSilhouetteInertiaUse cases\\nPredictive modeling, Spam detection\\nAnomaly detection, Customer segmentation\\nQ.21 What is linear regression, and What are the different assumptions of linear regression algorithms?Linear Regression – It is type of Supervised Learning where we compute a linear relationship between the predictor and response variable. It is based on the linear equation concept given by:\\n[Tex]\\\\hat{y} = \\\\beta_1x+\\\\beta_o\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex], where\\n[Tex]\\\\hat{y}\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = response / dependent variable[Tex]\\\\beta_1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = slope of the linear regression[Tex]\\\\beta_o\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = intercept for linear regression[Tex]x\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = predictor / independent variable(s) There are 4 assumptions we make about a Linear regression problem:\\nLinear relationship :\\xa0This assumes that there is a linear relationship between predictor and response variable. This means that, which changing values of predictor variable, the response variable changes linearly (either increases or decreases).Normality\\xa0: This assumes that the dataset is normally distributed, i.e., the data is symmetric about the mean of the dataset.Independence\\xa0: The features are independent of each other, there is no correlation among the features/predictor variables of the dataset.Homoscedasticity\\xa0: This assumes that the dataset has equal variance for all the predictor variables. This means that the amount of independent variables have no effect on the variance of data.Q.22 Logistic regression is a classification technique, why its name is regressions, not logistic classifications?While logistic regression is used for classification, it still maintains a regression structure underneath. The key idea is to model the probability of an event occurring (e.g., class 1 in binary classification) using a linear combination of features, and then apply a logistic (Sigmoid) function to transform this linear combination into a probability between 0 and 1. This transformation is what makes it suitable for classification tasks.\\nIn essence, while logistic regression is indeed used for classification, it retains the mathematical and structural characteristics of a regression model, hence the name.\\nQ.23 What is the logistic function (sigmoid function) in logistic regression?Sigmoid Function:\\xa0It is a mathematical function which is characterized by its S- shape curve. Sigmoid functions have the tendency to squash a data point to lie within 0 and 1. This is why it is also called Squashing function, which is given as:\\n\\n\\n\\n\\nSome of the properties of Sigmoid function is:\\nRange: [0,1]Q.24 What is overfitting and how can be overcome this?Overfitting refers to the result of analysis of a dataset which fits so closely with training data that it fails to generalize with unseen/future data. This happens when the model is trained with noisy data which causes it to learn the noisy features from the training as well.\\nTo avoid Overfitting and overcome this problem in machine learning, one can follow the following rules:\\nFeature selection :\\xa0Sometimes the training data has too many features which might not be necessary for our problem statement. In that case, we use only the necessary features that serve our purposeCross Validation :\\xa0This technique is a very powerful method to overcome overfitting. In this, the training dataset is divided into a set of mini training batches, which are used to tune the model.Regularization :\\xa0Regularization is the technique to supplement the loss with a penalty term so as to reduce overfitting. This penalty term regulates the overall loss function, thus creating a well trained model.Ensemble models :\\xa0These models learn the features and combine the results from different training models into a single prediction.Q.25 What is a support vector machine (SVM), and what are its key components?Support Vector machines are a type of Supervised algorithm which can be used for both Regression and Classification problems. In SVMs, the main goal is to find a hyperplane which will be used to segregate different data points into classes. Any new data point will be classified based on this defined hyperplane.\\nSupport Vector machines are highly effective when dealing with high dimensionality space and can handle non linear data very well. But if the number of features are greater than number of data samples, it is susceptible to overfitting.\\nThe key components of SVM are:\\nKernels Function: It is a mapping function used for data points to convert it into high dimensionality feature space.Hyperplane: It is the decision boundary which is used to differentiate between the classes of data points.Margin: It is the distance between Support Vector and HyperplaneC:\\xa0It is a regularization parameter which is used for margin maximization and misclassification minimization.Q.26 Explain the k-nearest neighbors (KNN) algorithm.The k-Nearest Neighbors (KNN) algorithm is a simple and versatile supervised machine learning algorithm used for both\\xa0classification and regression\\xa0tasks. KNN makes predictions by memorizing the data points rather than building a model about it. This is why it is also called “lazy learner” or “memory based” model too.\\nKNN relies on the principle that similar data points tend to belong to the same class or have similar target values. This means that, In the training phase, KNN stores the entire dataset consisting of feature vectors and their corresponding class labels (for classification) or target values (for regression). It then calculates the distances between that point and all the points in the training dataset. (commonly used distance metrics are Euclidean distance and Manhattan distance).\\n(Note : Choosing an appropriate value for k is crucial. A small k may result in noisy predictions, while a large k can smooth out the decision boundaries. The choice of distance metric and feature scaling also impact KNN’s performance.)\\nQ.27 What is the Naïve Bayes algorithm, what are the different assumptions of Naïve Bayes?The Naïve Bayes algorithm is a probabilistic classification algorithm based on Bayes’ theorem with a “naïve” assumption of feature independence within each class. It is commonly used for both binary and multi-class classification tasks, particularly in situations where simplicity, speed, and efficiency are essential.\\nThe main assumptions that Naïve Bayes theorem makes are:\\nFeature independence\\xa0– It assumes that the features involved in Naïve Bayes algorithm are conditionally independent, i.e., the presence/ absence of one feature does not affect any other featureEquality\\xa0– This assumes that the features are equal in terms of importance (or weight).Normality\\xa0– It assumes that the feature distribution is Normal in nature, i.e., the data is distributed equally around its mean.Q.28 What are decision trees, and how do they work?Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They work by creating a tree-like structure of decisions based on input features to make predictions or decisions. Lets dive into its core concepts and how they work briefly:\\nDecision trees consist of nodes and edges.The tree starts with a root node and branches into internal nodes that represent features or attributes.These nodes contain decision rules that split the data into subsets.Edges connect nodes and indicate the possible decisions or outcomes.Leaf nodes represent the final predictions or decisions.\\nThe objective is to increase data homogeneity, which is often measured using standards like mean squared error (for regression) or Gini impurity (for classification). Decision trees can handle a variety of attributes and can effectively capture complex data relationships. They can, however, overfit, especially when deep or complex. To reduce overfitting, strategies like pruning and restricting tree depth are applied.\\nQ.29 Explain the concepts of entropy and information gain in decision trees.Entropy: Entropy is the measure of randomness. In terms of Machine learning, Entropy can be defined as the measure of randomness or impurity in our dataset. It is given as:\\n, where\\n\\xa0= probability of an event “i”.\\nInformation gain:\\xa0It is defined as the change in the entropy of a feature given that there’s an additional information about that feature. If there are more than one features involved in Decision tree split, then the weighted average of entropies of the additional features is taken.\\nInformation gain =\\xa0, where\\nE = Entropy\\nQ.30 What is the difference between the bagging and boosting model?Category\\nBagging Model\\nBoosting model\\nDefinition\\nBagging, or Bootstrap aggregating, is an ensemble modelling method where predictions from different models are combined together to give the aggregated result\\nBoosting method is where multiple weak learners are used together to get a stronger model with more robust predictions.\\nAgenda\\nThis is used when dealing with models that have high variance (overfitting).\\nThis is used when dealing with models with high bias (underfitting) and variance as well.\\nRobustness to Noise and Sensitivity\\nThis is more robust due to averaging and this makes it less sensitive\\nIt is more sensitive to presence of outliers and that makes it a bit less robust as compared to bagging models\\nModel running and dependence\\nThe models are run in parallel and are typically independent\\nThe models are run in sequential method where the base model is dependent.\\nExamples\\nRandom Forest, Bagged Decision Trees\\nAdaBoost, Gradient Boosting, XGBoost\\nQ.31 Describe random forests and their advantages over single-decision trees.Random Forests are an ensemble learning technique that combines multiple decision trees to improve predictive accuracy and reduce overfitting. The advantages it has over single decision trees are:\\nImproved Generalization: Single decision trees are prone to overfitting, especially when they become deep and complex. Random Forests mitigate this issue by averaging predictions from multiple trees, resulting in a more generalized model that performs better on unseen dataBetter Handling of High-Dimensional Data :\\xa0Random Forests are effective at handling datasets with a large number of features. They select a random subset of features for each tree, which can improve the performance when there are many irrelevant or noisy featuresRobustness to Outliers:\\xa0Random Forests are more robust to outliers because they combine predictions from multiple trees, which can better handle extreme casesQ.32 What is K-Means, and how will it work?K-Means is an unsupervised machine learning algorithm used for clustering or grouping similar data points together. It aims to partition a dataset into K clusters, where each cluster represents a group of data points that are close to each other in terms of some similarity measure. The working of K-means is as follow:\\nChoose the number of clusters KFor each data point in the dataset, calculate its distance to each of the K centroids and then assign each data point to the cluster whose centroid is closest to itRecalculate the centroids of the K clusters based on the current assignment of data points.Repeat the above steps until a group of clusters are formed.Q.33 What is a confusion matrix? Explain with an example.Confusion matrix is a table used to evaluate the performance of a classification model by presenting a comprehensive view of the model’s predictions compared to the actual class labels. It provides valuable information for assessing the model’s accuracy, precision, recall, and other performance metrics in a binary or multi-class classification problem.\\nA famous example demonstration would be Cancer Confusion matrix:\\nActual\\nCancer\\nNot Cancer\\n\\nPredicted\\n\\nCancer\\nTrue Positive (TP)\\nFalse Positive (FP)\\nNot Cancer\\nFalse Negative (FN)\\nTrue Negative (TN)\\nTP (True Positive) = The number of instances correctly predicted as the positive classTN (True Negative) = The number of instances correctly predicted as the negative classFP (False Positive) = The number of instances incorrectly predicted as the positive classFN (False Negative) = The number of instances incorrectly predicted as the negative classQ.34 What is a classification report and explain the parameters used to interpret the result of classification tasks with an example.A classification report is a summary of the performance of a classification model, providing various metrics that help assess the quality of the model’s predictions on a classification task.\\nThe parameters used in a classification report typically include:\\nPrecision: Precision is the ratio of true positive predictions to the total predicted positives. It measures the accuracy of positive predictions made by the model.Precision = TP/(TP+FP)\\r\\nRecall (Sensitivity or True Positive Rate): Recall is the ratio of true positive predictions to the total actual positives. It measures the model’s ability to identify all positive instances correctly.Recall = TP / (TP + FN)\\r\\nAccuracy: Accuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. It measures the overall correctness of the model’s predictions.Accuracy = (TP + TN) / (TP + TN + FP + FN)\\r\\nF1-Score: The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure of both precision and recall and is particularly useful when dealing with imbalanced datasets.F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\\r\\nwhere,\\nTP = True PositiveTN = True NegativeFP = False PositiveFN = False NegativeIntermediate Data Science Interview Questions Q.35 Explain the uniform distribution.A fundamental probability distribution in statistics is the uniform distribution, commonly referred to as the rectangle distribution. A constant probability density function (PDF) across a limited range characterises it. In simpler terms, in a uniform distribution, every value within a specified range has an equal chance of occurring.\\nQ.36 Describe the Bernoulli distribution.A discrete probability distribution, the Bernoulli distribution is focused on discrete random variables. The number of heads you obtain while tossing three coins at once or the number of pupils in a class are examples of discrete random variables that have a finite or countable number of potential values.\\nQ.37 What is the binomial distribution?The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where each trial has only two possible outcomes: success or failure. The outcomes are often referred to as “success” and “failure,” but they can represent any dichotomous outcome, such as heads or tails, yes or no, or defective or non-defective.\\nThe fundamental presumptions of a binomial distribution are that each trial has exactly one possible outcome, each trial has an equal chance of success, and each trial is either independent of the others or mutually exclusive.\\nQ.38 Explain the exponential distribution and where it’s commonly used.The probability distribution of the amount of time between events in the Poisson point process is known as the exponential distribution. The gamma distribution is thought of as a particular instance of the exponential distribution. Additionally, the geometric distribution’s continuous analogue is the exponential distribution.\\nCommon applications of the exponential distribution include:\\nReliability EngineeringQueueing TheoryTelecommunicationsFinanceNatural PhenomenaSurvival AnalysisQ.39 Describe the Poisson distribution and its characteristics.The Poisson distribution is a probability distribution that describes the number of events that occur within a fixed interval of time or space when the events happen at a constant mean rate and are independent of the time since the last event. \\nKey characteristics of the Poisson distribution include:\\nDiscreteness: The Poisson distribution is used to model the number of discrete events that occur within a fixed interval.Constant Mean Rate: The events occur at a constant mean rate per unit of time or space.Independence: The occurrences of events are assumed to be independent of each other. The probability of multiple events occurring in a given interval is calculated based on the assumption of independence.Q40. Explain the t-distribution and its relationship with the normal distribution.The t-distribution, also known as the Student’s t-distribution, is used in statistics for inferences about population means when the sample size is small and the population standard deviation is unknown. The shape of the t-distribution is similar to the normal distribution, but it has heavier tails.\\nRelationship between T-Distribution and Normal Distribution: The t-distribution converges to the normal distribution as the degrees of freedom increase. In fact, when the degrees of freedom become very large, the t-distribution approaches the standard normal distribution (normal distribution with mean 0 and standard deviation 1). This is a result of the Central Limit Theorem.\\nQ.41 Describe the chi-squared distribution.The chi-squared distribution is a continuous probability distribution that arises in statistics and probability theory. It is commonly denoted as χ2 (chi-squared) and is associated with degrees of freedom. The chi-squared distribution is particularly used to model the distribution of the sum of squared independent standard normal random variables.It is also used to determine if data series are independent, the goodness of fit of a data distribution, and the level of confidence in the variance and standard deviation of a random variable with a normal distribution.\\nQ.42 What is the difference between z-test, F-test, and t-test?The z-test, t-test, and F-test are all statistical hypothesis tests used in different situations and for different purposes. Here’s a overview of each test and the key differences between them.\\nz-test\\nt-test\\nF-test\\nWhen we want to compare a sample mean to a known population mean and we know the population standard deviation, we use the z-test.\\nWhen we want to compare a sample mean to a known or assumed population mean but don’t know what the population standard deviation is we use the t-test.\\nThe F-test is used to compare the variances of two or more samples. It is commonly used in analysis of variance (ANOVA) and regression analysis.\\nWhen we dealing with large sample sizes or when we known the population standard deviation it is most frequently used.\\nThe t-test follows a t-distribution, which has different shapes depending on the degrees of freedom.\\nThe two-sample F-test, which analyses the variances of two independent samples, is the most popular of the F-test’s variants.\\nThe z-test follows a standard normal distribution when certain assumptions are met.\\nThe sample standard deviation (s) determines the test statistic for the t-test.\\nOne set of degrees of freedom corresponds to each sample’s degrees of freedom in the F-distribution.\\nIn summary, the choice between a z-test, t-test, or F-test depends on the specific research question and the characteristics of the data.\\nQ.43 What is the central limit theorem, and why is it significant in statistics?The Central Limit Theorem states that, regardless of the shape of the population distribution, the distribution of the sample means approaches a normal distribution as the sample size increases.This is true even if the population distribution is not normal. The larger the sample size, the closer the sampling distribution of the sample mean will be to a normal distribution.\\nQ.44 Describe the process of hypothesis testing, including null and alternative hypotheses.Hypothesis testing is a statistical method used to make inferences about population parameters based on sample data.It is a systematic way of evaluating statements or hypotheses about a population using observed sample data.To identify which statement is best supported by the sample data, it compares two statements about a population that are mutually exclusive.\\nNull hypothesis(H0):\\xa0The null hypothesis (H0) in statistics is the default assumption or assertion that there is no association between any two measured cases or any two groups. In other words, it is a fundamental assumption or one that is founded on knowledge of the problem.Alternative hypothesis(H1): The alternative hypothesis, or H1, is the null-hypothesis-rejecting hypothesis that is utilised in hypothesis testing.Q.45 How do you calculate a confidence interval, and what does it represent?A confidence interval (CI) is a statistical range or interval estimate for a population parameter, such as the population mean or population proportion, based on sample data. to calculate confidence interval these are the following steps.\\nCollect Sample DataChoose a Confidence LevelSelect the Appropriate Statistical MethodCalculate the Margin of Error (MOE)Calculate the Confidence IntervalInterpret the Confidence IntervalConfidence interval represents a range of values within which we believe, with a specified level of confidence (e.g., 95%), that the true population parameter lies.\\nQ.46 What is a p-value in Statistics?The term “p-value,” which stands for “probability value,” is a key one in statistics and hypothesis testing. It measures the evidence contradicting a null hypothesis and aids in determining whether a statistical test’s findings are statistically significant. Here is a definition of a p-value and how it is used in hypothesis testing.\\nQ.47 Explain Type I and Type II errors in hypothesis testing.Rejecting a null hypothesis that is actually true in the population results in a type I error (false-positive); failing to reject a null hypothesis that is actually untrue in the population results in a type II error (false-negative).\\ntype I and type II mistakes cannot be completely avoided, the investigator can lessen their risk by increasing the sample size (the less likely it is that the sample will significantly differ from the population).\\nQ.48 What is the significance level (alpha) in hypothesis testing?A crucial metric in hypothesis testing that establishes the bar for judging whether the outcomes of a statistical test are statistically significant is the significance level, which is sometimes indicated as (alpha). It reflects the greatest possible chance of committing a Type I error, or mistakenly rejecting a valid null hypothesis.\\nThe significance level in hypothesis testing.\\nSetting the Significance LevelInterpreting the Significance LevelHypothesis Testing Using Significance LevelChoice of Significance LevelQ.49 How can you calculate the correlation coefficient between two variables?The degree and direction of the linear link between two variables are quantified by the correlation coefficient. The Pearson correlation coefficient is the most widely used method for determining the correlation coefficient. The Pearson correlation coefficient can be calculated as follows.\\nCollect DataCalculate the MeansCalculate the CovarianceCalculate the Standard DeviationsCalculate the Pearson Correlation Coefficient (r)Interpret the Correlation Coefficient.Q.50 What is covariance, and how is it related to correlation?Both covariance and correlation are statistical metrics that show how two variables are related to one another.However, they serve slightly different purposes and have different interpretations.\\nCovariance\\xa0:Covariance measures the degree to which two variables change together. It expresses how much the values of one variable tend to rise or fall in relation to changes in the other variable.Correlation\\xa0: A standardised method for measuring the strength and direction of a linear relationship between two variables is correlation. It multiplies the standard deviations of the two variables to scale the covariance.Q.51 Explain how to perform a hypothesis test for comparing two population means.When comparing two population means, a hypothesis test is used to determine whether there is sufficient statistical support to claim that the means of the two distinct populations differ significantly. Tests we can commonly use for include  “paired t-test” or “two -sample t test”. The general procedures for carrying out such a test are as follows.\\nFormulate HypothesesChoose the Significance LevelCollect DataDefine Test Statistic Draw a ConclusionFinal ResultsQ.52 Explain the concept of normalization in database design.By minimising data duplication and enhancing data integrity, normalisation is a method in database architecture that aids in the effective organisation of data. It include dividing a big, complicated table into smaller, associated tables while making sure that connections between data elements are preserved. The basic objective of normalisation is to reduce data anomalies, which can happen when data is stored in an unorganised way and include insertion, update, and deletion anomalies.\\n\\xa0Q.53 What is database normalization?Database denormalization is the process of intentionally introducing redundancy into a relational database by merging tables or incorporating redundant data to enhance query performance. Unlike normalization, which minimizes data redundancy for consistency, denormalization prioritizes query speed. By reducing the number of joins required, denormalization can improve read performance for complex queries. However, it may lead to data inconsistencies and increased maintenance complexity. Denormalization is often employed in scenarios where read-intensive operations outweigh the importance of maintaining a fully normalized database structure. Careful consideration and trade-offs are essential to strike a balance between performance and data integrity.\\nQ.54 Define different types of SQL functions.SQL functions can be categorized into several types based on their functionality.\\nScalar FunctionsAggregate FunctionsWindow FunctionsTable-Valued FunctionsSystem FunctionsUser-Defined FunctionsConversion FunctionsConditional FunctionsQ.55 Explain the difference between INNER JOIN and LEFT JOIN.INNER JOIN and LEFT JOIN are two types of SQL JOIN operations used to combine data from multiple tables in a relational database. Here are the some main differences between them.\\nINNER JOIN\\nLEFT JOIN\\nOnly rows with a match in the designated columns between the two tables being connected are returned by an INNER JOIN.\\nLEFT JOIN returns all rows from the left table and the matching rows from the right table.\\nA row is not included in the result set if there is no match for it in either of the tables.\\nColumns from the right table’s rows are returned with NULL values if there is no match for that row.\\nWhen we want to retrieve data from both tables depending on a specific criterion, INNER JOIN can be helpful.\\nIt makes sure that every row from the left table appears in the final product, even if there are no matches for that row in the right table.\\nQ.56 What is a subquery, and how can it be used in SQL?A subquery is a query that is nested within another SQL query, also referred to as an inner query or nested query. On the basis of the outcomes of another query, we can use it to get data from one or more tables. SQL’s subqueries capability is employed for a variety of tasks, including data retrieval, computations, and filtering.\\nQ.57 How do you perform mathematical calculations in SQL queries?In SQL, we can perform mathematical calculations in queries using arithmetic operators and functions. Here are some common methods for performing mathematical calculations.\\nArithmetic OperatorsMathematical FunctionsAggregate FunctionsCustom ExpressionsQ.58 What is the purpose of the CASE statement in SQL?The SQL CASE statement is a flexible conditional expression that may be used to implement conditional logic inside of a query. we can specify various actions or values based on predetermined criteria.\\nQ.59 What is the difference between a database and a data warehouse?Database:\\xa0Consistency and real-time data processing are prioritised, and they are optimised for storing, retrieving, and managing structured data. Databases are frequently used for administrative functions like order processing, inventory control, and customer interactions.\\nData Warehouse:\\xa0Data warehouses are made for processing analytical data. They are designed to facilitate sophisticated querying and reporting by storing and processing massive amounts of historical data from various sources. Business intelligence, data analysis, and decision-making all employ data warehouses.\\nQ.60 What is regularization in machine learning, State the differences between L1 and L2 regularizationRegularization: Regularization is the technique to restrict the model overfitting during training by inducing a penalty to the loss. The penalty imposed on the loss function is added so that the complexity of the model can be controlled, thus overcoming the issue of overfitting in the model.\\nThe following are the differences between L1 and L2 regularization:\\ncategory\\nL1 Regularization(Lasso)\\nL2 Regularization (Ridge)\\nDefinition\\nL1 regularization is the technique where the induced penalty term changes some of the terms to be exactly zero\\nL2 regularization is the technique where the induced penalty term changes some of the terms to be as near to zero as possible.\\nInterpretability\\nSelects a subset of most important ones while eliminating less important ones.\\nSelects all the features but assigns less weights to less important features.\\nFormula\\n\\nwhere,\\nL1 = Lasso Loss function\\n\\xa0= Model loss\\n\\xa0= regularization controlling parameter\\nw = weights of the model\\n\\nwhere,\\nL2 = Ridge Loss function\\n\\xa0= Model loss\\n\\xa0= regularization controlling parameter\\nw = weights of the model\\nRobustness\\nSensitive to outliers and noisy data as it can eliminate them\\nMore robust to the presence of Outliers and noisy data\\nComputational efficiency\\nComputationally more expensive\\nComputationally less expensive.\\nQ.61 Explain the concepts of bias-variance trade-off in machine learning.When creating predictive models, the bias-variance trade-off is a key concept in machine learning that deals with finding the right balance between two sources of error, bias and variance. It plays a crucial role in model selection and understanding the generalization performance of a machine learning algorithm. Here’s an explanation of these concepts:\\nBias:Bias is simply described as the model’s inability to forecast the real value due of some difference or inaccuracy. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias.Variance: Variance is a measure of data dispersion from its mean location. In machine learning, variance is the amount by which a predictive model’s performance differs when trained on different subsets of the training data. More specifically, variance is the model’s variability in terms of how sensitive it is to another subset of the training dataset, i.e. how much it can adapt on the new subset of the training dataset.Low Bias\\nHigh Bias\\nLow Variance\\nBest fit (Ideal Scenario )\\nUnderfitting\\nHigh Variance\\nOverfitting\\nNot capture the underlying patterns(Worst Case)\\nAs a Data Science Professional, Our focus should be to achieve the the best fit model i.e Low Bias and Low Variance. A model with low bias and low variance suggests that it can capture the underlying patterns in the data (low bias) and is not overly sensitive to changes in the training data (low variance). This is the perfect circumstance for a machine learning model, since it can generalize effectively to new, previously unknown data and deliver consistent and accurate predictions. However, in practice, this is not achievable.\\n\\nIf the algorithm is too simplified (hypothesis with linear equation), it may be subject to high bias and low variance, making it error-prone. If algorithms fit too complicated a hypothesis (hypothesis with a high degree equation), it may have a large variance and a low bias. In the latter case, the new entries will underperform. There is, however, something in between these two situations called as a Trade-off or\\xa0Bias Variance Trade-off. So, that An algorithm can’t be more complex and less complex at the same time.\\nQ.62 How do we choose the appropriate kernel function in SVM?A kernel function is responsible for converting the original data points into a high dimensionality feature space. Choosing the appropriate kernel function in a Support Vector Machine is a crucial step, as it determines how well the SVM can capture the underlying patterns in your data. Below mentioned are some of the ways to choose the suitable kernel function:\\nIf the dataset exhibits linear relationshipIn this case, we should use Linear Kernel function. It is simple, computationally efficient and less prone to overfitting. For example, text classification, sentiment analysis, etc.\\nIf the dataset requires probabilistic approachThe sigmoid kernel is suitable when the data resembles a sigmoid function or when you have prior knowledge suggesting this shape. For example, Risk assessment, Financial applications, etc.\\nIf the dataset is Simple Non Linear in natureIn this case, use a Polynomial Kernel Function. Polynomial functions are useful when we are trying to capture moderate level of non linearity. For example, Image and Speech Recognition, etc.\\nIf the dataset is Highly Non-Linear in Nature/ we do not know about the underlying relationshipIn that case, a Radial basis function is the best choice. RBF kernel can handle highly complex dataset and is useful when you’re unsure about the data’s underlying distribution. For example, Financial forecasting, bioinformatics, etc.\\nQ.63 How does Naïve Bayes handle categorical and continuous features?Naive Bayes is probabilistic approach which assumes that the features are independent of each other. It calculates probabilities associated with each class label based on the observed frequencies of feature values within each class in the training data. This is done by finding the conditional probability of Feature given a class. (i.e., P(feature | class)). To make predictions on categorical data, Naive Bayes calculates the posterior probability of each class given the observed feature values and selects the class with the highest probability as the predicted class label. This is called as “maximum likelihood” estimation.\\nQ.64 What is Laplace smoothing (add-one smoothing) and why is it used in Naïve Bayes?In Naïve Bayes, the conditional probability of an event given a class label is determined as P(event| class). When using this in a classification problem (let’s say a text classification), there could a word which did not appear in the particular class. In those cases, the probability of feature given a class label will be zero. This could create a big problem when getting predictions out of the training data.\\nTo overcome this problem, we use Laplace smoothing. Laplace smoothing addresses the zero probability problem by adding a small constant (usually 1) to the count of each feature in each class and to the total count of features in each class. Without smoothing, if any feature is missing in a class, the probability of that class given the features becomes zero, making the classifier overly confident and potentially leading to incorrect classifications\\nQ.65 What are imbalanced datasets and how can we handle them?Imbalanced datasets are datasets in which the distribution of class labels (or target values) is heavily skewed, meaning that one class has significantly more instances than any other class. Imbalanced datasets pose challenges because models trained on such data can have a bias toward the majority class, leading to poor performance on the minority class, which is often of greater interest. This will lead to the model not generalizing well on the unseen data.\\nTo handle imbalanced datasets, we can approach the following methods:\\nResampling (Method of either increasing or decreasing the number of samples):Up-sampling: In this case, we can increase the classes for minority by either sampling without replacement or generating synthetic examples. Some of the popular examples are SMOTE (Synthetic Minority Over-sampling Technique), etc.Down-sampling: Another case would be to randomly cut down the majority class such that it is comparable to minority class.Ensemble methods (using models which are capable of handling imbalanced dataset inherently:Bagging\\xa0: Techniques like Random Forests, which can mitigate the impact of class imbalance by constructing multiple decision trees from bootstrapped samplesBoosting: Algorithms like AdaBoost and XGBoost can give more importance to misclassified minority class examples in each iteration, improving their representation in the final modelQ.66 What are outliers in the dataset and how can we detect and remove them?An Outlier is a data point that is significantly different from other data points. Usually, Outliers are present in the extremes of the distribution and stand out as compared to their out data point counterparts.\\nFor detecting Outliers we can use the following approaches:\\nVisual inspection:\\xa0This is the easiest way which involves plotting the data points into scatter plot/box plot, etc.statistics: By using measure of central tendency, we can determine if a data point falls significantly far from its mean, median, etc. making it a potential outlier.Z-score:\\xa0if a data point has very high Z-score, it can be identified as OutlierFor removing the outliers, we can use the following:\\nRemoval of outliers manuallyDoing transformations like applying logarithmic transformation or square rooting the outlierPerforming imputations wherein the outliers are replaced with different values like mean, median, mode, etc.Q.67 What is the curse of dimensionality And How can we overcome this?When dealing with a dataset that has high dimensionality (high number of features), we are often encountered with various issues and problems. Some of the issues faced while dealing with dimensionality dataset are listed below:\\nComputational expense: The biggest problem with handling a dataset with vast number of features is that it takes a long time to process and train the model on it. This can lead to wastage of both time and monetary resources.Data sparsity: Many times data points are far from each other (high sparsity). This makes it harder to find the underlying patterns between features and can be a hinderance in proper analysisVisualising issues and overfitting: It is rather easy to visualize 2d and 3d data. But beyond this order, it is difficult to properly visualize our data. Furthermore, more data features can be correlated and provide misleading information to the model training and cause overfitting.These issues are what are generally termed as “Curse of Dimensionality”.\\nTo overcome this, we can follow different approaches – some of which are mentioned below:\\nFeature Selection: Many a times, not all the features are necessary. It is the user’s job to select out the features that would be necessary in solving a given problem statement.Feature engineering: Sometimes, we may need a feature that is the combination of many other features. This method can, in general, reduces the features count in the dataset.Dimensionality Reduction techniques: These techniques reduce the number of features in a dataset while preserving as much useful information as possible. Some of the famous Dimensionality reduction techniques are: Principle component analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), etc.Regularization:\\xa0Some regularization techniques like L1 and L2 regularizations are useful when deciding the impact each feature has on the model training.Q.68 How does the random forest algorithm handle feature selection?Mentioned below is how Random forest handles feature selection\\nWhen creating individual trees in the Random Forest ensemble, a subset of features is assigned to each tree which is called Feature Bagging. Feature Bagging introduces randomness and diversity among the trees.After the training, the features are assigned a “importance score” based on how well those features performed by reducing the error of the model. Features that consistently contribute to improving the model’s accuracy across multiple trees are deemed more importantThen the features are ranked based on their importance scores. Features with higher importance scores are considered more influential in making predictions.Q.69 What is feature engineering? Explain the different feature engineering methods.Feature Engineering: It can be defined as a method of preprocessing of data for better analysis purpose which involves different steps like selection, transformation, deletion of features to suit our problem at hand. Feature Engineering is a useful tool which can be used for:\\nImproving the model’s performance and Data interpretabilityReduce computational costsInclude hidden patterns for elevated Analysis results.Some of the different methods of doing feature engineering are mentioned below:\\nPrinciple Component Analysis (PCA)\\xa0: It identifies orthogonal axes (principal components) in the data that capture the maximum variance, thereby reducing the data features.Encoding\\xa0– It is a technique of converting the data to be represented a numbers with some meaning behind it. It can be done in two ways :One-Hot Encoding\\xa0– When we need to encode Nominal Categorical DataLabel Encoding\\xa0– When we need to encode Ordinal Categorical DataFeature Transformation: Sometimes, we can create new columns essential for better modelling just by combining or modifying one or more columns.Q.70 How we will deal with the categorical text values in machine learning?Often times, we are encountered with data that has Categorical text values. For example, male/female, first-class/second-class/third-class, etc. These Categorical text values can be divided into two types and based on that we deal with them as follows:\\nIf it is Categorical Nominal Data: If the data does not have any hidden order associated with it (e.g., male/female), we perform One-Hot encoding on the data to convert it into binary sequence of digitsIf it is Categorical Ordinal Data : When there is a pattern associated with the text data, we use Label encoding. In this, the numerical conversion is done based on the order of the text data. (e.g., Elementary/ Middle/ High/ Graduate,etc.)Q.71 What is DBSCAN and How we will use it?Density-Based Spatial Clustering of Applications with Noise (DBSCAN), is a density-based clustering algorithm used for grouping together data points that are close to each other in high-density regions and labeling data points in low-density regions as outliers or noise. Here is how it works:\\nFor each data point in the dataset, DBSCAN calculates the distance between that point and all other data pointsDBSCAN identifies dense regions by connecting core points that are within each other’s predefined threshold (eps) neighborhood.DBSCAN forms clusters by grouping together data points that are density-reachable from one another.Q.72 How does the EM (Expectation-Maximization) algorithm work in clustering?The Expectation-Maximization (EM) algorithm is a probabilistic approach used for clustering data when dealing with mixture models. EM is commonly used when the true cluster assignments are not known and when there is uncertainty about which cluster a data point belongs to. Here is how it works:\\nFirst, the number of clusters K to be formed is specified.Then, for each data point, the likelihood of it belonging to each of the K clusters is calculated. This is called the Expectation (E) stepBased on the previous step, the model parameters are updated. This is called Maximization (M) step.Together it is used to check for convergence by comparing the change in log-likelihood or the parameter values between iterations.If it converges, then we have achieved our purpose. If not, then the E-step and M-step are repeated until we reach convergence.Q.73 Explain the concept of silhouette score in clustering evaluation.Silhouette score is a metric used to evaluate the quality of clusters produced by a clustering algorithm. Here is how it works:\\nthe average distance between the data point and all other data points in the same cluster is first calculated. Let us call this as (a)Then for the same data point, the average distance (b) between the data point and all data points in the nearest neighboring cluster (i.e., the cluster to which it is not assigned)silhouette coefficient for each data point is calculated, which given by: S = (b – a) / max(a, b)if -1<S<0, it signifies that data point is closer to a neighboring cluster than to its own cluster.if S is close to zero, data point is on or very close to the decision boundary between two neighboring clusters.if 0<S<1, data point is well within its own cluster and far from neighboring clusters.Q.74 What is the relationship between eigenvalues and eigenvectors in PCA?In Principal Component Analysis (PCA), eigenvalues and eigenvectors play a crucial role in the transformation of the original data into a new coordinate system. Let us first define the essential terms:\\nEigen Values: Eigenvalues are associated with each eigenvector and represent the magnitude of the variance (spread or extent) of the data along the corresponding eigenvectorEigen Vectors: Eigenvectors are the directions or axes in the original feature space along which the data varies the most or exhibits the most varianceThe relationship between them is given as:\\n[Tex]AV = \\\\lambda{V}\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex], where\\nA = Feature matrix\\nV = eigen vector\\n[Tex]\\\\lambda\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = Eigen value.\\nA larger eigenvalue implies that the corresponding eigenvector captures more of the variance in the data.The sum of all eigenvalues equals the total variance in the original data. Therefore, the proportion of total variance explained by each principal component can be calculated by dividing its eigenvalue by the sum of all eigenvalues\\nQ.75 What is the cross-validation technique in machine learning?Cross-validation is a resampling technique used in machine learning to assess and validate the performance of a predictive model. It helps in estimating how well a model is likely to perform on unseen data, making it a crucial step in model evaluation and selection. Cross validation is usually helpful when avoiding overfitting the model. Some of the widely known cross validation techniques are:\\nK-Fold Cross-Validation: In this, the data is divided into K subsets, and K iterations of training and testing are performed.Stratified K-Fold Cross-Validation: This technique ensures that each fold has approximately the same proportion of classes as the original dataset (helpful in handling data imbalance)Shuffle-Split Cross-Validation: It randomly shuffles the data and splits it into training and testing sets.Q.76 What are the ROC and AUC, explain its significance in binary classification.Receiver Operating Characteristic (ROC) is a graphical representation of a binary classifier’s performance. It plots the true positive rate (TPR) vs the false positive rate (FPR) at different classification thresholds.\\nTrue positive rate (TPR) : It is the ratio of true positive predictions to the total actual positives.\\nRecall = TP / (TP + FN)\\r\\nFalse positive rate (FPR) : It is the ratio of False positive predictions to the total actual positives.\\nFPR= FP / (TP + FN)\\r\\n\\nArea Under the Curve (AUC) as the name suggests is the area under the ROC curve. The AUC is a scalar value that quantifies the overall performance of a binary classification model and ranges from 0 to 1, where a model with an AUC of 0.5 indicates random guessing, and an AUC of 1 represents a perfect classifier.\\nQ.77 Describe gradient descent and its role in optimizing machine learning models.Gradient descent is a fundamental optimization algorithm used to minimize a cost or loss function in machine learning and deep learning. Its primary role is to iteratively adjust the parameters of a machine learning model to find the values that minimize the cost function, thereby improving the model’s predictive performance. Here’s how Gradient descent help in optimizing Machine learning models:\\nMinimizing Cost functions: The primary goal of gradient descent is to find parameter values that result in the lowest possible loss on the training data.Convergence: The algorithm continues to iterate and update the parameters until it meets a predefined convergence criterion, which can be a maximum number of iterations or achieving a desired level of accuracy.Generalization: Gradient descent ensure that the optimized model generalizes well to new, unseen data.Q.78 Describe batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.Batch Gradient Descent:\\xa0In Batch Gradient Descent, the entire training dataset is used to compute the gradient of the cost function with respect to the model parameters (weights and biases) in each iteration. This means that all training examples are processed before a single parameter update is made. It converges to a more accurate minimum of the cost function but can be slow, especially in a high dimensionality space.\\nStochastic Gradient Descent:\\xa0In Stochastic Gradient Descent, only one randomly selected training example is used to compute the gradient and update the parameters in each iteration. The selection of examples is done independently for each iteration. This is capable of faster updates and can handle large datasets because it processes one example at a time but high variance can cause it to converge slower.\\nMini-Batch Gradient Descent:\\xa0Mini-Batch Gradient Descent strikes a balance between BGD and SGD. It divides the training dataset into small, equally-sized subsets called mini-batches. In each iteration, a mini-batch is randomly sampled, and the gradient is computed based on this mini-batch. It utilizes parallelism well and takes advantage of modern hardware like GPUs but can still exhibits some level of variance in updates compared to Batch Gradient Descent.\\nQ.79 Explain the Apriori — Association Rule MiningAssociation Rule mining is an algorithm to find relation between two or more different objects. Apriori association is one of the most frequently used and most simple association technique. Apriori Association uses prior knowledge of frequent objects properties. It is based on Apriori property which states that:\\n“All non-empty subsets of a frequent itemset must also be frequent”\\nData Science Interview Questions for ExperiencedQ.80 Explain multivariate distribution in data science.A vector with several normally distributed variables is said to have a multivariate normal distribution if any linear combination of the variables likewise has a normal distribution. The multivariate normal distribution is used to approximatively represent the features of specific characteristics in machine learning, but it is also important in extending the central limit theorem to several variables.\\nQ.81 Describe the concept of conditional probability density function (PDF).In probability theory and statistics, the conditional probability density function (PDF) is a notion that represents the probability distribution of a random variable within a certain condition or constraint. It measures the probability of a random variable having a given set of values given a set of circumstances or events.\\nQ.82 What is the cumulative distribution function (CDF), and how is it related to PDF?The probability that a continuous random variable will take on particular values within a range is described by the Probability Density Function (PDF), whereas the Cumulative Distribution Function (CDF) provides the cumulative probability that the random variable will fall below a given value. Both of these concepts are used in probability theory and statistics to describe and analyse probability distributions. The PDF is the CDF’s derivative, and they are related by integration and differentiation.\\nQ.83 What is ANOVA? What are the different ways to perform ANOVA tests?The statistical method known as ANOVA, or Analysis of Variance, is used to examine the variation in a dataset and determine whether there are statistically significant variations between group averages. When comparing the means of several groups or treatments to find out if there are any notable differences, this method is frequently used.\\nThere are several different ways to perform ANOVA tests, each suited for different types of experimental designs and data structures:\\nOne-Way ANOVATwo-Way ANOVAThree-Way ANOVAWhen conducting ANOVA tests we typically calculate an F-statistic and compare it to a critical value or use it to calculate a p-value.\\nQ.84 How can you prevent gradient descent from getting stuck in local minima?Ans:\\xa0The local minima problem occurs when the optimization algorithm converges a solution that is minimum within a small neighbourhood of the current point but may not be the global minimum for the objective function.\\nTo mitigate local minimal problems, we can use the following technique:\\nUse initialization techniques like Xavier/Glorot and He to model trainable parameters. This will help to set appropriate initial weights for the optimization process.Set Adam or RMSProp as optimizer, these adaptive learning rate algorithms can adapt the learning rates for individual parameters based on historical gradients.Introduce stochasticity in the optimization process using mini-batches, which can help the optimizer to escape local minima by adding noise to the gradient estimates.Adding more layers or neurons can create a more complex loss landscape with fewer local minima.Hyperparameter tuning using random search cv and grid search cv helps to explore the parameter space more thoroughly suggesting right hyperparameters for training and reducing the risk of getting stuck in local minima.Q.85 Explain the Gradient Boosting algorithms in machine learning.Gradient boosting techniques like XGBoost, and CatBoost are used for regression and classification problems. It is a boosting algorithm that combines the predictions of weak learners to create a strong model. The key steps involved in gradient boosting are:\\nInitialize the model with weak learners, such as a decision tree.Calculate the difference between the target value and predicted value made by the current model.Add a new weak learner to calculate residuals and capture the errors made by the current ensemble.Update the model by adding fraction of the new weak learner’s predictions. This updating process can be controlled by learning rate.Repeat the process from step 2 to 4, with each iteration focusing on correcting the errors made by the previous model.Q.86\\xa0Explain convolutions operations of CNN architecture?In a CNN architecture, convolution operations involve applying small filters (also called kernels) to input data to extract features. These filters slide over the input image covering one small part of the input at a time, computing dot products at each position creating a feature map. This operation captures the similarity between the filter’s pattern and the local features in the input. Strides determine how much the filter moves between positions. The resulting feature maps capture patterns, such as edges, textures, or shapes, and are essential for image recognition tasks. Convolution operations help reduce the spatial dimensions of the data and make the network translation-invariant, allowing it to recognize features in different parts of an image. Pooling layers are often used after convolutions to further reduce dimensions and retain important information.\\nQ.87\\xa0What is feed forward network and how it is different from recurrent neural network?Deep learning designs that are basic are feedforward neural networks and recurrent neural networks. They are both employed for different tasks, but their structure and how they handle sequential data differ.\\nFeed Forward Neural Network\\nIn FFNN, the information flows in one direction, from input to output, with no loopsIt consists of multiple layers of neurons, typically organized into an input layer, one or more hidden layers, and an output layer.Each neuron in a layer is connected to every neuron in the subsequent layer through weighted connections.FNNs are primarily used for tasks such as classification and regression, where they take a fixed-size input and produce a corresponding outputRecurrent Neural Network\\nA recurrent neural network is designed to handle sequential data, where the order of input elements matters. Unlike FNNs, RNNs have connections that loop back on themselves, allowing them to maintain a hidden state that carries information from previous time steps.This hidden state enables RNNs to capture temporal dependencies and context in sequential data, making them well-suited for tasks like natural language processing, time series analysis, and sequence generation.However, standard RNNs have limitations in capturing long-range dependencies due to the vanishing gradient problem.Q.88 Explain the difference between generative and discriminative models?Generative models focus on generating new data samples, while discriminative models concentrate on classification and prediction tasks based on input data.\\nGenerative Models:\\nObjective: Model the joint probability distribution P(X, Y) of input X and target Y.Use: Generate new data, often for tasks like image and text generation.Examples: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs).Discriminative Models:\\nObjective: Model the conditional probability distribution P(Y | X) of target Y given input X.Use: Classify or make predictions based on input data.Examples: Logistic Regression, Support Vector Machines, Convolutional Neural Networks (CNNs) for image classification.Q.89 What is the forward and backward propogations in deep learning?Forward and backward propagations are key processes that occur during neural network training in deep learning. They are essential for optimizing network parameters and learning meaningful representations from input.\\nThe process by which input data is passed through the neural network to generate predictions or outputs is known as forward propagation. The procedure begins at the input layer, where data is fed into the network. Each neuron in a layer calculates the weighted total of its inputs, applies an activation function, and sends the result to the next layer. This process continues through the hidden layers until the final output layer produces predictions or scores for the given input data.\\nThe technique of computing gradients of the loss function with regard to the network’s parameters is known as backward propagation. It is utilized to adjust the neural network parameters during training using optimization methods such as gradient descent.\\nThe process starts with the computation of the loss, which measures the difference between the network’s predictions and the actual target values. Gradients are then computed by using the chain rule of calculus to propagate this loss backward through the network. This entails figuring out how much each parameter contributed to the error. The computed gradients are used to adjust the network’s weights and biases, reducing the error in subsequent forward passes.\\nQ.90 Describe the use of Markov models in sequential data analysis?Markov models are effective methods for capturing and modeling dependencies between successive data points or states in a sequence. They are especially useful when the current condition is dependent on earlier states. The Markov property, which asserts that the future state or observation depends on the current state and is independent of all prior states. There are two types of Markov models used in sequential data analysis:\\nMarkov chains are the simplest form of Markov models, consisting of a set of states and transition probabilities between these states. Each state represents a possible condition or observation, and the transition probabilities describe the likelihood of moving from one state to another.Hidden Markov Models extend the concept of Markov chains by introducing a hidden layer of states and observable emissions associated with each hidden state. The true state of the system (hidden state) is not directly observable, but the emissions are observable.Applications:\\nHMMs are used to model phonemes and words in speech recognition systems, allowing for accurate transcription of spoken languageHMMs are applied in genomics for gene prediction and sequence alignment tasks. They can identify genes within DNA sequences and align sequences for evolutionary analysis.Markov models are used in modeling financial time series data, such as stock prices, to capture the dependencies between consecutive observations and make predictions.Q.91 What is generative AI?Generative AI is an abbreviation for Generative Artificial Intelligence, which refers to a class of artificial intelligence systems and algorithms that are designed to generate new, unique data or material that is comparable to, or indistinguishable from, human-created data. It is a subset of artificial intelligence that focuses on the creative component of AI, allowing machines to develop innovative outputs such as writing, graphics, audio, and more. There are several generative AI models and methodologies, each adapted to different sorts of data and applications such as:\\nGenerative AI models such as GPT (Generative Pretrained Transformer) can generate human-like text.” Natural language synthesis, automated content production, and chatbot responses are all common uses for these models.Images are generated using generative adversarial networks (GANs).” GANs are made up of a generator network that generates images and a discriminator network that determines the authenticity of the generated images. Because of the struggle between the generator and discriminator, high-quality, realistic images are produced.Generative AI can also create audio content, such as speech synthesis and music composition.” Audio content is generated using models such as WaveGAN and Magenta.Q.92 What are different neural network architecture used to generate artificial data in deep learning?Various neural networks are used to generate artificial data. Here are some of the neural network architectures used for generating artificial data:\\nGANs consist of two components – generator and discriminator, which are trained simultaneously through adversarial training. They are used to generating high-quality images, such as photorealistic faces, artwork, and even entire scenes.VAEs are generative models that learn a probabilistic mapping from the data space to a latent space. They also consist of encoder and decoder. They are used for generating images, reconstructing missing parts of images, and generating new data samples. They are also applied in generating text and audio.RNNs are a class of neural networks with recurrent connections that can generate sequences of data. They are often used for sequence-to-sequence tasks. They are used in text generation, speech synthesis, music composition.Transformers are a type of neural network architecture that has gained popularity for sequence-to-sequence tasks. They use self-attention mechanisms to capture dependencies between different positions in the input data. They are used in natural language processing tasks like machine translation, text summarization, and language generation.Autoencoders are neural networks that are trained to reconstruct their input data. Variants like denoising autoencoders and contractive autoencoders can be used for data generation. They are used for image denoising, data inpainting, and generating new data samples.Q.93 What is deep reinforcement learning technique?Deep Reinforcement Learning (DRL) is a cutting-edge machine learning technique that combines the principles of reinforcement learning with the capability of deep neural networks. Its ability to enable machines to learn difficult tasks independently by interacting with their environments, similar to how people learn via trial and error, has garnered significant attention.\\nDRL is made up of three fundamental components:\\nThe agent interacts with the environment and takes decision.The environment is the outside world with which the agent interacts and receives feedback.The reward signal is a scalar value provided by the environment after each action, guiding the agent toward maximizing cumulative rewards over time.Applications:\\nIn robotics, DRL is used to control robots, manipulation and navigation.DRL plays a role in self-driving cars and vehicle controlCan also be used for customized recommendationsQ.94 What is transfer learning, and how is it applied in deep learning?Transfer learning is a strong machine learning and deep learning technique that allows models to apply knowledge obtained from one task or domain to a new, but related. It is motivated by the notion that what we learn in one setting can be applied to a new, but comparable, challenge.\\nBenefits of Transfer Learning:\\nWe may utilize knowledge from a large dataset by starting with a pretrained model, making it easier to adapt to a new task with\\xa0data.Training a deep neural network from scratch can be time-consuming and costly in terms of compute. Transfer learning enables us to bypass the earliest phases of training, saving both time and resources.Pretrained models frequently learn rich data representations. Models that use these representations can generalize better, even when the target task has a smaller dataset.Transfer Learning Process:\\nFeature ExtractionIt’s a foundation step in transfer learning. The pretrained data is already trained on large and diverse dataset for a related task.To leverage the knowlege, output layers of the pretrained model are removed leaving the layers responsible for feature extraction. The target data is passed through these layers to extract feature information.using these extracted features, the model captures patterns and representations from the data.Fine TuningAfter the feature extraction process, the model is fine-tuned for the specific target task.Output layers are added to the model and these layer are designed to produce the desired output for the target task.Backpropagation is used to iteratively update the model’s weights during fine-tuning. This method allows the model to tailor its representations and decision boundaries to the specifics of the target task.Even as the model focuses in the target task, the knowledge and features learned from the pre-trained layers continue to contribute to its understanding. This dual learning process improves the model’s performance and enables it to thrive in tasks that require little data or resources.Q.95 What is difference between object detections and image segmentations.Object detection and Image\\xa0segmentation are both computer vision tasks that entail evaluating and comprehending image content, but they serve different functions and give different sorts of information.\\nObject Detection:\\ngoal of object detection is to identify and locate objects and represent the object in bounding boxes with their respective labels.used in applications like autonomous driving for detecting pedestrians and vehicleImage Segmentation:\\nfocuses on partitioning an image into multiple regions, where each segment corresponding to a coherent part of the image.provide pixel level labeling of the entire imageused in applications that require pixel level understanding such as medical image analysis for organ and tumor delineation.Q.96 Explain the concept of word embeddings in natural language processing (NLP).In NLP, the concept of word embedding is use to capture semantic and contextual information. Word embeddings are dense representations of words or phrases in continuous-valued vectors in a high-dimensional space. Each word is mapped to a vector with the real numbers, these vectors are learned from large corpora of text data.\\nWord embeddings are based on the Distributional Hypothesis, which suggests that words that appear in similar context\\xa0have similar meanings. This idea is used by word embedding models to generate vector representations that reflect the semantic links between words depending on how frequently they co-occur with other words in the\\xa0text.\\nThe most common word embeddings techniques are-\\nBag of Words (BOW)Word2VecGlove: Global Vector for word representationTerm frequency-inverse document frequency (TF-IDF)BERTQ.97 What is seq2seq model?A neural network architecture called a Sequence-to-Sequence (Seq2Seq) model is made to cope with data sequences, making it particularly helpful for jobs involving variable-length input and output sequences. Machine translation, text summarization, question answering, and other tasks all benefit from its extensive use in natural language processing.\\nThe Seq2Seq consists of two main components: encoder and decoder. The encoder takes input sequence and converts into fixed length vector . The vector captures features and context of the sequence. The decoder takes the vector as input and generated output sequence. This autoregressive technique frequently entails influencing the subsequent prediction using the preceding one.\\nQ.98\\xa0What is artificial neural networks.Artificial neural networks take inspiration from structure and functioning of human brain. The computational units in ANN are called neurons and these neurons are responsible to process and pass the information to the next layer.\\nANN has three main components:\\nInput Layer: where the network receives input features.Hidden Layer:\\xa0one or more layers of interconnected neurons responsible for learning patterns in the dataOutput Layer: provides final output on processed information.Q.99 What is marginal probability?A key idea in statistics and probability theory is marginal probability, which is also known as marginal distribution. With reference to a certain variable of interest, it is the likelihood that an event will occur, without taking into account the results of other variables. Basically, it treats the other variables as if they were “marginal” or irrelevant and concentrates on one.\\nMarginal probabilities are essential in many statistical analyses, including estimating anticipated values, computing conditional probabilities, and drawing conclusions about certain variables of interest while taking other variables’ influences into account.\\nQ.100 What are the probability axioms?The fundamental rules that control the behaviour and characteristics of probabilities in probability theory and statistics are referred to as the probability axioms, sometimes known as the probability laws or probability principles.\\nThere are three fundamental axioms of probability:\\nNon-Negativity AxiomNormalization AxiomAdditivity AxiomConclusionWe all know that data science is growing career and if you are looking a future in data science, then explore this detailed article on data science interview questions. \\n \\n\\n\\nLike Article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest improvement\\n\\n\\n\\n\\n  \\n\\nNext\\n\\n\\n\\n\\nData Science Interview Questions and Answers\\n\\n\\n\\n\\n\\nShare your thoughts in the comments\\n\\n\\nAdd Your Comment\\n\\n \\n\\n\\n\\n\\n\\n  Please Login to comment...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                    Similar Reads                                    \\r\\n                                \\n\\n\\n\\r\\n                DIKW Pyramid | Data, Information, Knowledge and Wisdom | Data Science and Big Data Analytics\\r\\n            \\n\\n\\n\\r\\n                Data Analyst Interview Questions and Answers\\r\\n            \\n\\n\\n\\r\\n                Top 50 Data Mining Interview Questions & Answers\\r\\n            \\n\\n\\n\\r\\n                Ethics in Data Science and Proper Privacy and Usage of Data\\r\\n            \\n\\n\\n\\r\\n                Top 50 TCP/IP interview questions and answers\\r\\n            \\n\\n\\n\\r\\n                Top 50 IP addressing interview questions and answers\\r\\n            \\n\\n\\n\\r\\n                Top 50 Flutter Interview Questions and Answers (2023)\\r\\n            \\n\\n\\n\\r\\n                Top 25 Android Interview Questions and Answers For Experienced\\r\\n            \\n\\n\\n\\r\\n                Merge Sort Interview Questions and Answers\\r\\n            \\n\\n\\n\\r\\n                Sales Interview Questions and Answers\\r\\n            \\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\npawan_kumar_gunjan \\n\\n\\n\\n\\n\\n Follow \\n\\n\\nArticle Tags : \\n\\n\\nAI-ML-DS With Python\\n\\n\\ninterview-questions\\n\\n\\nAI-ML-DS\\n\\n\\nData Science\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n248k+ interested Geeks \\n\\n\\n\\nComplete Machine Learning & Data Science Program \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n35k+ interested Geeks \\n\\n\\n\\nGATE Data Science and Artificial Intelligence 2025 \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n255k+ interested Geeks \\n\\n\\n\\nData Structures & Algorithms in Python - Self Paced \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n \\nExplore More\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                     A-143, 9th Floor, Sovereign Corporate Tower, Sector-136, Noida, Uttar Pradesh - 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompanyAbout UsLegalCareersIn MediaContact UsAdvertise with usGFG Corporate SolutionPlacement Training ProgramExploreJob-A-Thon Hiring ChallengeHack-A-ThonGfG Weekly ContestOffline Classes (Delhi/NCR)DSA in JAVA/C++Master System DesignMaster CPGeeksforGeeks VideosGeeks CommunityLanguagesPythonJavaC++PHPGoLangSQLR LanguageAndroid TutorialDSAData StructuresAlgorithmsDSA for BeginnersBasic DSA ProblemsDSA RoadmapDSA Interview QuestionsCompetitive ProgrammingData Science & MLData Science With PythonData Science For BeginnerMachine Learning TutorialML MathsData Visualisation TutorialPandas TutorialNumPy TutorialNLP TutorialDeep Learning TutorialWeb TechnologiesHTMLCSSJavaScriptTypeScriptReactJSNextJSNodeJsBootstrapTailwind CSSPython TutorialPython Programming ExamplesDjango TutorialPython ProjectsPython TkinterWeb ScrapingOpenCV TutorialPython Interview QuestionComputer ScienceGATE CS NotesOperating SystemsComputer NetworkDatabase Management SystemSoftware EngineeringDigital Logic DesignEngineering MathsDevOpsGitAWSDockerKubernetesAzureGCPDevOps RoadmapSystem DesignHigh Level DesignLow Level DesignUML DiagramsInterview GuideDesign PatternsOOADSystem Design BootcampInterview QuestionsSchool SubjectsMathematicsPhysicsChemistryBiologySocial ScienceEnglish GrammarCommerceAccountancyBusiness StudiesEconomicsManagementHR ManagementFinanceIncome TaxUPSC Study MaterialPolity NotesGeography NotesHistory NotesScience and Technology NotesEconomy NotesEthics NotesPrevious Year PapersPreparation CornerCompany-Wise Recruitment ProcessResume TemplatesAptitude PreparationPuzzlesCompany-Wise PreparationCompaniesCollegesCompetitive ExamsJEE AdvancedUGC NETSSC CGLSBI POSBI ClerkIBPS POIBPS ClerkMore TutorialsSoftware DevelopmentSoftware TestingProduct ManagementProject ManagementLinuxExcelAll Cheat SheetsFree Online ToolsTyping TestImage EditorCode FormattersCode ConvertersCurrency ConverterRandom Number GeneratorRandom Password GeneratorWrite & EarnWrite an ArticleImprove an ArticlePick Topics to WriteShare your ExperiencesInternships \\n\\n \\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        We use cookies to ensure you have the best browsing experience on our website. By using our site, you\\r\\n        acknowledge that you have read and understood our\\r\\n        Cookie Policy &\\r\\n        Privacy Policy\\n\\n\\r\\n        Got It !\\r\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\nThis article is being improved by another user right now. You can suggest the changes for now and it will be under the article's discussion tab.\\nYou will be notified via email once the article is available for improvement.\\r\\n                        Thank you for your valuable feedback!\\r\\n                    \\n\\nSuggest changes\\n\\n\\n\\nPlease go through our recently updated Improvement Guidelines before submitting any improvements.\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\n\\nSuggestion[CharLimit:2000]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Compass for School Students\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n                        Can't choose a topic to write? click here for suggested topics\\n                    \\n\\n\\n\\n                       Write and publish your own Article\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/', 'title': 'Top 100+ Data Science Interview Questions and Answers (2024)', 'description': 'Prepare for your next data science interview with confidence using our comprehensive list of Top 100+ Data Science Interview Questions and Answers (2024). Covering a wide range of topics from basic concepts to advanced techniques, this resource will help you ace your interview and land your dream job in data science.', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "#No.2 Web Loder defult directly load all page\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/\")\n",
    "data=loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\nData Science Interview Questions and Answers\\n\\nData Science Interview Questions – Explore the Data Science Interview Questions and Answers for beginners and experienced professionals looking for new opportunities in data science. \\n\\nWe all know that data science is a field where data scientists mine raw data, analyze it, and extract useful insights from it. The article outlines the frequently asked questionas during the data science interview. Practising all the below questions will help you to explore your career as a data scientist.\\nTable of Content\\nBasic Data Science Interview Questions For FresherIntermediate Data Science Interview Questions Data Science Interview Questions for ExperiencedWhat is Data Science?Data science is a field that extracts knowledge and insights from structured and unstructured data by using scientific methods, algorithms, processes, and systems. It combines expertise from various domains, such as statistics, computer science, machine learning, data engineering, and domain-specific knowledge, to analyze and interpret complex data sets.\\nFurthermore, data scientists use a combination of multiple languages, such as Python and R. They are also frequent users of data analysis tools like pandas, NumPy, and scikit-learn, as well as machine learning libraries.\\nAfter exploring the brief of data science, let’s dig into the data science interview questions and answers.\\nBasic Data Science Interview Questions For FresherQ.1 What is marginal probability?A key idea in statistics and probability theory is marginal probability, which is also known as marginal distribution. With reference to a certain variable of interest, it is the likelihood that an event will occur, without taking into account the results of other variables. Basically, it treats the other variables as if they were “marginal” or irrelevant and concentrates on one.\\nMarginal probabilities are essential in many statistical analyses, including estimating anticipated values, computing conditional probabilities, and drawing conclusions about certain variables of interest while taking other variables’ influences into account.\\nQ.2 What are the probability axioms?The fundamental rules that control the behaviour and characteristics of probabilities in probability theory and statistics are referred to as the probability axioms, sometimes known as the probability laws or probability principles.\\nThere are three fundamental axioms of probability:\\nNon-Negativity AxiomNormalization AxiomAdditivity AxiomQ.3 What is conditional probability?The event or outcome occurring based on the existence of a prior event or outcome is known as conditional probability. It is determined by multiplying the probability of the earlier occurrence by the increased lprobability of the later, or conditional, event.\\nQ.4 What is Bayes’ Theorem and when is it used in data science?The Bayes theorem predicts the probability that an event connected to any condition would occur. It is also taken into account in the situation of conditional probability. The probability of “causes” formula is another name for the Bayes theorem.\\nIn data science, Bayes’ Theorem is used primarily in:\\nBayesian InferenceMachine LearningText ClassificationMedical DiagnosisPredictive ModelingWhen working with ambiguous or sparse data, Bayes’ Theorem is very helpful since it enables data scientists to continually revise their assumptions and come to more sensible conclusions.\\nQ.5 Define variance and conditional variance.A statistical concept known as variance quantifies the spread or dispersion of a group of data points within a dataset. It sheds light on how widely individual data points depart from the dataset’s mean (average). It assesses the variability or “scatter” of data.\\nConditional Variance\\nA measure of the dispersion or variability of a random variable under certain circumstances or in the presence of a particular event, as the name implies. It reflects a random variable’s variance that is dependent on the knowledge of another random variable’s variance.\\nQ.6 Explain the concepts of mean, median, mode, and standard deviation.Mean:\\xa0The mean, often referred to as the average, is calculated by summing up all the values in a dataset and then dividing by the total number of values.\\nMedian:\\xa0When data are sorted in either ascending or descending order, the median is the value in the middle of the dataset. The median is the average of the two middle values when the number of data points is even.In comparison to the mean, the median is less impacted by extreme numbers, making it a more reliable indicator of central tendency.\\nMode:\\xa0The value that appears most frequently in a dataset is the mode. One mode (unimodal), several modes (multimodal), or no mode (if all values occur with the same frequency) can all exist in a dataset.\\nStandard deviation: The spread or dispersion of data points in a dataset is measured by the standard deviation. It quantifies the variance between different data points.\\nQ.7 What is the normal distribution and standard normal distribution?The normal distribution, also known as the Gaussian distribution or bell curve, is a continuous probability distribution that is characterized by its symmetric bell-shaped curve. The normal distribution is defined by two parameters: the mean (μ) and the standard deviation (σ). The mean determines the center of the distribution, and the standard deviation determines the spread or dispersion of the distribution. The distribution is symmetric around its mean, and the bell curve is centered at the mean. The probabilities for values that are further from the mean taper off equally in both directions. Similar rarity applies to extreme values in the two tails of the distribution. Not all symmetrical distributions are normal, even though the normal distribution is symmetrical.\\nThe standard normal distribution, also known as the Z distribution, is a special case of the normal distribution where the mean (μ) is 0 and the standard deviation (σ) is 1. It is a standardized form of the normal distribution, allowing for easy comparison of scores or observations from different normal distributions.\\nQ.8\\xa0What is SQL,\\xa0and what does it stand for?SQL stands for Structured Query Language.It is a specialized programming language used for managing and manipulating relational databases. It is designed for tasks related to database management, data retrieval, data manipulation, and data definition.\\nQ.9 Explain the differences between SQL and NoSQL databases.Both\\xa0SQL\\xa0(Structured Query Language) and NoSQL (Not Only SQL) databases, differ in their data structures, schema, query languages, and use cases. The following are the main variations between SQL and NoSQL databases.\\nSQL\\nNoSQL\\nSQL databases are relational databases, they organise and store data using a structured schema with tables, rows, and columns.\\nNoSQL databases use a number of different types of data models, such as document-based (like JSON and BSON), key-value pairs, column families, and graphs.\\nSQL databases have a set schema, thus before inserting data, we must establish the structure of our data.The schema may need to be changed, which might be a difficult process.\\nNoSQL databases frequently employ a dynamic or schema-less approach, enabling you to insert data without first creating a predetermined schema.\\nSQL is a strong and standardised query language that is used by SQL databases. Joins, aggregations, and subqueries are only a few of the complicated processes supported by SQL queries.\\nThe query languages or APIs used by NoSQL databases are frequently tailored to the data model.\\nQ.10 What are the primary SQL database management systems (DBMS)?Relational database systems, both open source and commercial, are the main SQL (Structured Query Language) database management systems (DBMS), which are widely used for managing and processing structured data. Some of the most popular SQL database management systems are listed below:\\nMySQLMicrosoft SQL ServerSQLitePostgreSQLOracle DatabaseAmazon RDSQ.11 What is the ER model in SQL?The structure and relationships between the data entities in a database are represented by the Entity-Relationship (ER) model, a conceptual framework used in database architecture. The ER model is frequently used in conjunction with SQL for creating the structure of relational databases even though it is not a component of the SQL language itself.\\nQ.12 What is data transformation?The process of transforming data from one structure, format, or representation into another is referred to as data transformation. In order to make the data more suited for a given goal, such as analysis, visualisation, reporting, or storage, this procedure may involve a variety of actions and changes to the data. Data integration, cleansing, and analysis depend heavily on data transformation, which is a common stage in data preparation and processing pipelines.\\nQ.13 What are the main components of a SQL query?A relational database’s data can be retrieved, modified, or managed via a SQL (Structured Query Language) query. The operation of a SQL query is defined by a number of essential components, each of which serves a different function.\\nSELECTFROMWHEREGROUP BYHAVINGORDER BYLIMITJOINQ.14 What is a primary key?A relational database table’s main key, also known as a primary keyword, is a column that is unique for each record. It is a distinctive identifier.The primary key of a relational database must be unique. Every row of data must have a primary key value and none of the rows can be null.\\nQ.15 What is the purpose of the GROUP BY clause, and how is it used?In SQL, the GROUP BY clause is used to create summary rows out of rows that have the same values in a set of specified columns. In order to do computations on groups of rows as opposed to individual rows, it is frequently used in conjunction with aggregate functions like SUM, COUNT, AVG, MAX, or MIN. we may produce summary reports and perform more in-depth data analysis using the GROUP BY clause.\\nQ.16 What is the WHERE clause used for, and how is it used to filter data?In SQL, the WHERE clause is used to filter rows from a table or result set according to predetermined criteria. It enables us to pick only the rows that satisfy particular requirements or follow a pattern. A key element of SQL queries, the WHERE clause is frequently used for data retrieval and manipulation.\\nQ.17 How do you retrieve distinct values from a column in SQL?Using the DISTINCT keyword in combination with the SELECT command, we can extract distinct values from a column in SQL. By filtering out duplicate values and returning only unique values from the specified column, the DISTINCT keyword is used.\\nQ.18 What is the HAVING clause?To filter query results depending on the output of aggregation functions, the HAVING clause, a SQL clause, is used along with the GROUP BY clause. The HAVING clause filters groups of rows after they have been grouped by one or more columns, in contrast to the WHERE clause, which filters rows before they are grouped.\\nQ.19 How do you handle missing or NULL values in a database table?Missing or NULL values can arise due to various reasons, such as incomplete data entry, optional fields, or data extraction processes.\\nReplace NULL with Placeholder ValuesHandle NULL Values in QueriesUse Default ValuesQ.20 What is the difference between supervised and unsupervised machine learning?The difference between Supervised Learning and Unsupervised Learning are as follow:\\nCategory\\nSupervised Learning\\nUnsupervised Learning\\nDefinition\\nSupervised learning refers to that part of machine learning where we know what the target variable is and it is labeled.\\nUnsupervised Learning is used when we do not have labeled data and we are not sure about our target variable\\nObjective\\nThe objective of supervised learning is to predict an outcome or classify the data\\nThe objective here is to discover patterns among the features of the dataset and group similar features together\\nAlgorithms\\nSome of the algorithm types are:\\nRegression (Linear, Logistic, etc.)Classification (Decision Tree Classifier, Support Vector Classifier, etc.)Some of the algorithms are :\\nDimensionality reduction (Principle Component Analysis, etc.)Clustering (KMeans, DBSCAN, etc.)Evaluation metrics\\nSupervised learning uses evaluation metrics like:\\nMean Squared ErrorAccuracyUnsupervised Learning uses evaluation metrics like:\\nSilhouetteInertiaUse cases\\nPredictive modeling, Spam detection\\nAnomaly detection, Customer segmentation\\nQ.21 What is linear regression, and What are the different assumptions of linear regression algorithms?Linear Regression – It is type of Supervised Learning where we compute a linear relationship between the predictor and response variable. It is based on the linear equation concept given by:\\n[Tex]\\\\hat{y} = \\\\beta_1x+\\\\beta_o\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex], where\\n[Tex]\\\\hat{y}\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = response / dependent variable[Tex]\\\\beta_1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = slope of the linear regression[Tex]\\\\beta_o\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = intercept for linear regression[Tex]x\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = predictor / independent variable(s) There are 4 assumptions we make about a Linear regression problem:\\nLinear relationship :\\xa0This assumes that there is a linear relationship between predictor and response variable. This means that, which changing values of predictor variable, the response variable changes linearly (either increases or decreases).Normality\\xa0: This assumes that the dataset is normally distributed, i.e., the data is symmetric about the mean of the dataset.Independence\\xa0: The features are independent of each other, there is no correlation among the features/predictor variables of the dataset.Homoscedasticity\\xa0: This assumes that the dataset has equal variance for all the predictor variables. This means that the amount of independent variables have no effect on the variance of data.Q.22 Logistic regression is a classification technique, why its name is regressions, not logistic classifications?While logistic regression is used for classification, it still maintains a regression structure underneath. The key idea is to model the probability of an event occurring (e.g., class 1 in binary classification) using a linear combination of features, and then apply a logistic (Sigmoid) function to transform this linear combination into a probability between 0 and 1. This transformation is what makes it suitable for classification tasks.\\nIn essence, while logistic regression is indeed used for classification, it retains the mathematical and structural characteristics of a regression model, hence the name.\\nQ.23 What is the logistic function (sigmoid function) in logistic regression?Sigmoid Function:\\xa0It is a mathematical function which is characterized by its S- shape curve. Sigmoid functions have the tendency to squash a data point to lie within 0 and 1. This is why it is also called Squashing function, which is given as:\\n\\n\\n\\n\\nSome of the properties of Sigmoid function is:\\nRange: [0,1]Q.24 What is overfitting and how can be overcome this?Overfitting refers to the result of analysis of a dataset which fits so closely with training data that it fails to generalize with unseen/future data. This happens when the model is trained with noisy data which causes it to learn the noisy features from the training as well.\\nTo avoid Overfitting and overcome this problem in machine learning, one can follow the following rules:\\nFeature selection :\\xa0Sometimes the training data has too many features which might not be necessary for our problem statement. In that case, we use only the necessary features that serve our purposeCross Validation :\\xa0This technique is a very powerful method to overcome overfitting. In this, the training dataset is divided into a set of mini training batches, which are used to tune the model.Regularization :\\xa0Regularization is the technique to supplement the loss with a penalty term so as to reduce overfitting. This penalty term regulates the overall loss function, thus creating a well trained model.Ensemble models :\\xa0These models learn the features and combine the results from different training models into a single prediction.Q.25 What is a support vector machine (SVM), and what are its key components?Support Vector machines are a type of Supervised algorithm which can be used for both Regression and Classification problems. In SVMs, the main goal is to find a hyperplane which will be used to segregate different data points into classes. Any new data point will be classified based on this defined hyperplane.\\nSupport Vector machines are highly effective when dealing with high dimensionality space and can handle non linear data very well. But if the number of features are greater than number of data samples, it is susceptible to overfitting.\\nThe key components of SVM are:\\nKernels Function: It is a mapping function used for data points to convert it into high dimensionality feature space.Hyperplane: It is the decision boundary which is used to differentiate between the classes of data points.Margin: It is the distance between Support Vector and HyperplaneC:\\xa0It is a regularization parameter which is used for margin maximization and misclassification minimization.Q.26 Explain the k-nearest neighbors (KNN) algorithm.The k-Nearest Neighbors (KNN) algorithm is a simple and versatile supervised machine learning algorithm used for both\\xa0classification and regression\\xa0tasks. KNN makes predictions by memorizing the data points rather than building a model about it. This is why it is also called “lazy learner” or “memory based” model too.\\nKNN relies on the principle that similar data points tend to belong to the same class or have similar target values. This means that, In the training phase, KNN stores the entire dataset consisting of feature vectors and their corresponding class labels (for classification) or target values (for regression). It then calculates the distances between that point and all the points in the training dataset. (commonly used distance metrics are Euclidean distance and Manhattan distance).\\n(Note : Choosing an appropriate value for k is crucial. A small k may result in noisy predictions, while a large k can smooth out the decision boundaries. The choice of distance metric and feature scaling also impact KNN’s performance.)\\nQ.27 What is the Naïve Bayes algorithm, what are the different assumptions of Naïve Bayes?The Naïve Bayes algorithm is a probabilistic classification algorithm based on Bayes’ theorem with a “naïve” assumption of feature independence within each class. It is commonly used for both binary and multi-class classification tasks, particularly in situations where simplicity, speed, and efficiency are essential.\\nThe main assumptions that Naïve Bayes theorem makes are:\\nFeature independence\\xa0– It assumes that the features involved in Naïve Bayes algorithm are conditionally independent, i.e., the presence/ absence of one feature does not affect any other featureEquality\\xa0– This assumes that the features are equal in terms of importance (or weight).Normality\\xa0– It assumes that the feature distribution is Normal in nature, i.e., the data is distributed equally around its mean.Q.28 What are decision trees, and how do they work?Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They work by creating a tree-like structure of decisions based on input features to make predictions or decisions. Lets dive into its core concepts and how they work briefly:\\nDecision trees consist of nodes and edges.The tree starts with a root node and branches into internal nodes that represent features or attributes.These nodes contain decision rules that split the data into subsets.Edges connect nodes and indicate the possible decisions or outcomes.Leaf nodes represent the final predictions or decisions.\\nThe objective is to increase data homogeneity, which is often measured using standards like mean squared error (for regression) or Gini impurity (for classification). Decision trees can handle a variety of attributes and can effectively capture complex data relationships. They can, however, overfit, especially when deep or complex. To reduce overfitting, strategies like pruning and restricting tree depth are applied.\\nQ.29 Explain the concepts of entropy and information gain in decision trees.Entropy: Entropy is the measure of randomness. In terms of Machine learning, Entropy can be defined as the measure of randomness or impurity in our dataset. It is given as:\\n, where\\n\\xa0= probability of an event “i”.\\nInformation gain:\\xa0It is defined as the change in the entropy of a feature given that there’s an additional information about that feature. If there are more than one features involved in Decision tree split, then the weighted average of entropies of the additional features is taken.\\nInformation gain =\\xa0, where\\nE = Entropy\\nQ.30 What is the difference between the bagging and boosting model?Category\\nBagging Model\\nBoosting model\\nDefinition\\nBagging, or Bootstrap aggregating, is an ensemble modelling method where predictions from different models are combined together to give the aggregated result\\nBoosting method is where multiple weak learners are used together to get a stronger model with more robust predictions.\\nAgenda\\nThis is used when dealing with models that have high variance (overfitting).\\nThis is used when dealing with models with high bias (underfitting) and variance as well.\\nRobustness to Noise and Sensitivity\\nThis is more robust due to averaging and this makes it less sensitive\\nIt is more sensitive to presence of outliers and that makes it a bit less robust as compared to bagging models\\nModel running and dependence\\nThe models are run in parallel and are typically independent\\nThe models are run in sequential method where the base model is dependent.\\nExamples\\nRandom Forest, Bagged Decision Trees\\nAdaBoost, Gradient Boosting, XGBoost\\nQ.31 Describe random forests and their advantages over single-decision trees.Random Forests are an ensemble learning technique that combines multiple decision trees to improve predictive accuracy and reduce overfitting. The advantages it has over single decision trees are:\\nImproved Generalization: Single decision trees are prone to overfitting, especially when they become deep and complex. Random Forests mitigate this issue by averaging predictions from multiple trees, resulting in a more generalized model that performs better on unseen dataBetter Handling of High-Dimensional Data :\\xa0Random Forests are effective at handling datasets with a large number of features. They select a random subset of features for each tree, which can improve the performance when there are many irrelevant or noisy featuresRobustness to Outliers:\\xa0Random Forests are more robust to outliers because they combine predictions from multiple trees, which can better handle extreme casesQ.32 What is K-Means, and how will it work?K-Means is an unsupervised machine learning algorithm used for clustering or grouping similar data points together. It aims to partition a dataset into K clusters, where each cluster represents a group of data points that are close to each other in terms of some similarity measure. The working of K-means is as follow:\\nChoose the number of clusters KFor each data point in the dataset, calculate its distance to each of the K centroids and then assign each data point to the cluster whose centroid is closest to itRecalculate the centroids of the K clusters based on the current assignment of data points.Repeat the above steps until a group of clusters are formed.Q.33 What is a confusion matrix? Explain with an example.Confusion matrix is a table used to evaluate the performance of a classification model by presenting a comprehensive view of the model’s predictions compared to the actual class labels. It provides valuable information for assessing the model’s accuracy, precision, recall, and other performance metrics in a binary or multi-class classification problem.\\nA famous example demonstration would be Cancer Confusion matrix:\\nActual\\nCancer\\nNot Cancer\\n\\nPredicted\\n\\nCancer\\nTrue Positive (TP)\\nFalse Positive (FP)\\nNot Cancer\\nFalse Negative (FN)\\nTrue Negative (TN)\\nTP (True Positive) = The number of instances correctly predicted as the positive classTN (True Negative) = The number of instances correctly predicted as the negative classFP (False Positive) = The number of instances incorrectly predicted as the positive classFN (False Negative) = The number of instances incorrectly predicted as the negative classQ.34 What is a classification report and explain the parameters used to interpret the result of classification tasks with an example.A classification report is a summary of the performance of a classification model, providing various metrics that help assess the quality of the model’s predictions on a classification task.\\nThe parameters used in a classification report typically include:\\nPrecision: Precision is the ratio of true positive predictions to the total predicted positives. It measures the accuracy of positive predictions made by the model.Precision = TP/(TP+FP)\\r\\nRecall (Sensitivity or True Positive Rate): Recall is the ratio of true positive predictions to the total actual positives. It measures the model’s ability to identify all positive instances correctly.Recall = TP / (TP + FN)\\r\\nAccuracy: Accuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. It measures the overall correctness of the model’s predictions.Accuracy = (TP + TN) / (TP + TN + FP + FN)\\r\\nF1-Score: The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure of both precision and recall and is particularly useful when dealing with imbalanced datasets.F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\\r\\nwhere,\\nTP = True PositiveTN = True NegativeFP = False PositiveFN = False NegativeIntermediate Data Science Interview Questions Q.35 Explain the uniform distribution.A fundamental probability distribution in statistics is the uniform distribution, commonly referred to as the rectangle distribution. A constant probability density function (PDF) across a limited range characterises it. In simpler terms, in a uniform distribution, every value within a specified range has an equal chance of occurring.\\nQ.36 Describe the Bernoulli distribution.A discrete probability distribution, the Bernoulli distribution is focused on discrete random variables. The number of heads you obtain while tossing three coins at once or the number of pupils in a class are examples of discrete random variables that have a finite or countable number of potential values.\\nQ.37 What is the binomial distribution?The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where each trial has only two possible outcomes: success or failure. The outcomes are often referred to as “success” and “failure,” but they can represent any dichotomous outcome, such as heads or tails, yes or no, or defective or non-defective.\\nThe fundamental presumptions of a binomial distribution are that each trial has exactly one possible outcome, each trial has an equal chance of success, and each trial is either independent of the others or mutually exclusive.\\nQ.38 Explain the exponential distribution and where it’s commonly used.The probability distribution of the amount of time between events in the Poisson point process is known as the exponential distribution. The gamma distribution is thought of as a particular instance of the exponential distribution. Additionally, the geometric distribution’s continuous analogue is the exponential distribution.\\nCommon applications of the exponential distribution include:\\nReliability EngineeringQueueing TheoryTelecommunicationsFinanceNatural PhenomenaSurvival AnalysisQ.39 Describe the Poisson distribution and its characteristics.The Poisson distribution is a probability distribution that describes the number of events that occur within a fixed interval of time or space when the events happen at a constant mean rate and are independent of the time since the last event. \\nKey characteristics of the Poisson distribution include:\\nDiscreteness: The Poisson distribution is used to model the number of discrete events that occur within a fixed interval.Constant Mean Rate: The events occur at a constant mean rate per unit of time or space.Independence: The occurrences of events are assumed to be independent of each other. The probability of multiple events occurring in a given interval is calculated based on the assumption of independence.Q40. Explain the t-distribution and its relationship with the normal distribution.The t-distribution, also known as the Student’s t-distribution, is used in statistics for inferences about population means when the sample size is small and the population standard deviation is unknown. The shape of the t-distribution is similar to the normal distribution, but it has heavier tails.\\nRelationship between T-Distribution and Normal Distribution: The t-distribution converges to the normal distribution as the degrees of freedom increase. In fact, when the degrees of freedom become very large, the t-distribution approaches the standard normal distribution (normal distribution with mean 0 and standard deviation 1). This is a result of the Central Limit Theorem.\\nQ.41 Describe the chi-squared distribution.The chi-squared distribution is a continuous probability distribution that arises in statistics and probability theory. It is commonly denoted as χ2 (chi-squared) and is associated with degrees of freedom. The chi-squared distribution is particularly used to model the distribution of the sum of squared independent standard normal random variables.It is also used to determine if data series are independent, the goodness of fit of a data distribution, and the level of confidence in the variance and standard deviation of a random variable with a normal distribution.\\nQ.42 What is the difference between z-test, F-test, and t-test?The z-test, t-test, and F-test are all statistical hypothesis tests used in different situations and for different purposes. Here’s a overview of each test and the key differences between them.\\nz-test\\nt-test\\nF-test\\nWhen we want to compare a sample mean to a known population mean and we know the population standard deviation, we use the z-test.\\nWhen we want to compare a sample mean to a known or assumed population mean but don’t know what the population standard deviation is we use the t-test.\\nThe F-test is used to compare the variances of two or more samples. It is commonly used in analysis of variance (ANOVA) and regression analysis.\\nWhen we dealing with large sample sizes or when we known the population standard deviation it is most frequently used.\\nThe t-test follows a t-distribution, which has different shapes depending on the degrees of freedom.\\nThe two-sample F-test, which analyses the variances of two independent samples, is the most popular of the F-test’s variants.\\nThe z-test follows a standard normal distribution when certain assumptions are met.\\nThe sample standard deviation (s) determines the test statistic for the t-test.\\nOne set of degrees of freedom corresponds to each sample’s degrees of freedom in the F-distribution.\\nIn summary, the choice between a z-test, t-test, or F-test depends on the specific research question and the characteristics of the data.\\nQ.43 What is the central limit theorem, and why is it significant in statistics?The Central Limit Theorem states that, regardless of the shape of the population distribution, the distribution of the sample means approaches a normal distribution as the sample size increases.This is true even if the population distribution is not normal. The larger the sample size, the closer the sampling distribution of the sample mean will be to a normal distribution.\\nQ.44 Describe the process of hypothesis testing, including null and alternative hypotheses.Hypothesis testing is a statistical method used to make inferences about population parameters based on sample data.It is a systematic way of evaluating statements or hypotheses about a population using observed sample data.To identify which statement is best supported by the sample data, it compares two statements about a population that are mutually exclusive.\\nNull hypothesis(H0):\\xa0The null hypothesis (H0) in statistics is the default assumption or assertion that there is no association between any two measured cases or any two groups. In other words, it is a fundamental assumption or one that is founded on knowledge of the problem.Alternative hypothesis(H1): The alternative hypothesis, or H1, is the null-hypothesis-rejecting hypothesis that is utilised in hypothesis testing.Q.45 How do you calculate a confidence interval, and what does it represent?A confidence interval (CI) is a statistical range or interval estimate for a population parameter, such as the population mean or population proportion, based on sample data. to calculate confidence interval these are the following steps.\\nCollect Sample DataChoose a Confidence LevelSelect the Appropriate Statistical MethodCalculate the Margin of Error (MOE)Calculate the Confidence IntervalInterpret the Confidence IntervalConfidence interval represents a range of values within which we believe, with a specified level of confidence (e.g., 95%), that the true population parameter lies.\\nQ.46 What is a p-value in Statistics?The term “p-value,” which stands for “probability value,” is a key one in statistics and hypothesis testing. It measures the evidence contradicting a null hypothesis and aids in determining whether a statistical test’s findings are statistically significant. Here is a definition of a p-value and how it is used in hypothesis testing.\\nQ.47 Explain Type I and Type II errors in hypothesis testing.Rejecting a null hypothesis that is actually true in the population results in a type I error (false-positive); failing to reject a null hypothesis that is actually untrue in the population results in a type II error (false-negative).\\ntype I and type II mistakes cannot be completely avoided, the investigator can lessen their risk by increasing the sample size (the less likely it is that the sample will significantly differ from the population).\\nQ.48 What is the significance level (alpha) in hypothesis testing?A crucial metric in hypothesis testing that establishes the bar for judging whether the outcomes of a statistical test are statistically significant is the significance level, which is sometimes indicated as (alpha). It reflects the greatest possible chance of committing a Type I error, or mistakenly rejecting a valid null hypothesis.\\nThe significance level in hypothesis testing.\\nSetting the Significance LevelInterpreting the Significance LevelHypothesis Testing Using Significance LevelChoice of Significance LevelQ.49 How can you calculate the correlation coefficient between two variables?The degree and direction of the linear link between two variables are quantified by the correlation coefficient. The Pearson correlation coefficient is the most widely used method for determining the correlation coefficient. The Pearson correlation coefficient can be calculated as follows.\\nCollect DataCalculate the MeansCalculate the CovarianceCalculate the Standard DeviationsCalculate the Pearson Correlation Coefficient (r)Interpret the Correlation Coefficient.Q.50 What is covariance, and how is it related to correlation?Both covariance and correlation are statistical metrics that show how two variables are related to one another.However, they serve slightly different purposes and have different interpretations.\\nCovariance\\xa0:Covariance measures the degree to which two variables change together. It expresses how much the values of one variable tend to rise or fall in relation to changes in the other variable.Correlation\\xa0: A standardised method for measuring the strength and direction of a linear relationship between two variables is correlation. It multiplies the standard deviations of the two variables to scale the covariance.Q.51 Explain how to perform a hypothesis test for comparing two population means.When comparing two population means, a hypothesis test is used to determine whether there is sufficient statistical support to claim that the means of the two distinct populations differ significantly. Tests we can commonly use for include  “paired t-test” or “two -sample t test”. The general procedures for carrying out such a test are as follows.\\nFormulate HypothesesChoose the Significance LevelCollect DataDefine Test Statistic Draw a ConclusionFinal ResultsQ.52 Explain the concept of normalization in database design.By minimising data duplication and enhancing data integrity, normalisation is a method in database architecture that aids in the effective organisation of data. It include dividing a big, complicated table into smaller, associated tables while making sure that connections between data elements are preserved. The basic objective of normalisation is to reduce data anomalies, which can happen when data is stored in an unorganised way and include insertion, update, and deletion anomalies.\\n\\xa0Q.53 What is database normalization?Database denormalization is the process of intentionally introducing redundancy into a relational database by merging tables or incorporating redundant data to enhance query performance. Unlike normalization, which minimizes data redundancy for consistency, denormalization prioritizes query speed. By reducing the number of joins required, denormalization can improve read performance for complex queries. However, it may lead to data inconsistencies and increased maintenance complexity. Denormalization is often employed in scenarios where read-intensive operations outweigh the importance of maintaining a fully normalized database structure. Careful consideration and trade-offs are essential to strike a balance between performance and data integrity.\\nQ.54 Define different types of SQL functions.SQL functions can be categorized into several types based on their functionality.\\nScalar FunctionsAggregate FunctionsWindow FunctionsTable-Valued FunctionsSystem FunctionsUser-Defined FunctionsConversion FunctionsConditional FunctionsQ.55 Explain the difference between INNER JOIN and LEFT JOIN.INNER JOIN and LEFT JOIN are two types of SQL JOIN operations used to combine data from multiple tables in a relational database. Here are the some main differences between them.\\nINNER JOIN\\nLEFT JOIN\\nOnly rows with a match in the designated columns between the two tables being connected are returned by an INNER JOIN.\\nLEFT JOIN returns all rows from the left table and the matching rows from the right table.\\nA row is not included in the result set if there is no match for it in either of the tables.\\nColumns from the right table’s rows are returned with NULL values if there is no match for that row.\\nWhen we want to retrieve data from both tables depending on a specific criterion, INNER JOIN can be helpful.\\nIt makes sure that every row from the left table appears in the final product, even if there are no matches for that row in the right table.\\nQ.56 What is a subquery, and how can it be used in SQL?A subquery is a query that is nested within another SQL query, also referred to as an inner query or nested query. On the basis of the outcomes of another query, we can use it to get data from one or more tables. SQL’s subqueries capability is employed for a variety of tasks, including data retrieval, computations, and filtering.\\nQ.57 How do you perform mathematical calculations in SQL queries?In SQL, we can perform mathematical calculations in queries using arithmetic operators and functions. Here are some common methods for performing mathematical calculations.\\nArithmetic OperatorsMathematical FunctionsAggregate FunctionsCustom ExpressionsQ.58 What is the purpose of the CASE statement in SQL?The SQL CASE statement is a flexible conditional expression that may be used to implement conditional logic inside of a query. we can specify various actions or values based on predetermined criteria.\\nQ.59 What is the difference between a database and a data warehouse?Database:\\xa0Consistency and real-time data processing are prioritised, and they are optimised for storing, retrieving, and managing structured data. Databases are frequently used for administrative functions like order processing, inventory control, and customer interactions.\\nData Warehouse:\\xa0Data warehouses are made for processing analytical data. They are designed to facilitate sophisticated querying and reporting by storing and processing massive amounts of historical data from various sources. Business intelligence, data analysis, and decision-making all employ data warehouses.\\nQ.60 What is regularization in machine learning, State the differences between L1 and L2 regularizationRegularization: Regularization is the technique to restrict the model overfitting during training by inducing a penalty to the loss. The penalty imposed on the loss function is added so that the complexity of the model can be controlled, thus overcoming the issue of overfitting in the model.\\nThe following are the differences between L1 and L2 regularization:\\ncategory\\nL1 Regularization(Lasso)\\nL2 Regularization (Ridge)\\nDefinition\\nL1 regularization is the technique where the induced penalty term changes some of the terms to be exactly zero\\nL2 regularization is the technique where the induced penalty term changes some of the terms to be as near to zero as possible.\\nInterpretability\\nSelects a subset of most important ones while eliminating less important ones.\\nSelects all the features but assigns less weights to less important features.\\nFormula\\n\\nwhere,\\nL1 = Lasso Loss function\\n\\xa0= Model loss\\n\\xa0= regularization controlling parameter\\nw = weights of the model\\n\\nwhere,\\nL2 = Ridge Loss function\\n\\xa0= Model loss\\n\\xa0= regularization controlling parameter\\nw = weights of the model\\nRobustness\\nSensitive to outliers and noisy data as it can eliminate them\\nMore robust to the presence of Outliers and noisy data\\nComputational efficiency\\nComputationally more expensive\\nComputationally less expensive.\\nQ.61 Explain the concepts of bias-variance trade-off in machine learning.When creating predictive models, the bias-variance trade-off is a key concept in machine learning that deals with finding the right balance between two sources of error, bias and variance. It plays a crucial role in model selection and understanding the generalization performance of a machine learning algorithm. Here’s an explanation of these concepts:\\nBias:Bias is simply described as the model’s inability to forecast the real value due of some difference or inaccuracy. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias.Variance: Variance is a measure of data dispersion from its mean location. In machine learning, variance is the amount by which a predictive model’s performance differs when trained on different subsets of the training data. More specifically, variance is the model’s variability in terms of how sensitive it is to another subset of the training dataset, i.e. how much it can adapt on the new subset of the training dataset.Low Bias\\nHigh Bias\\nLow Variance\\nBest fit (Ideal Scenario )\\nUnderfitting\\nHigh Variance\\nOverfitting\\nNot capture the underlying patterns(Worst Case)\\nAs a Data Science Professional, Our focus should be to achieve the the best fit model i.e Low Bias and Low Variance. A model with low bias and low variance suggests that it can capture the underlying patterns in the data (low bias) and is not overly sensitive to changes in the training data (low variance). This is the perfect circumstance for a machine learning model, since it can generalize effectively to new, previously unknown data and deliver consistent and accurate predictions. However, in practice, this is not achievable.\\n\\nIf the algorithm is too simplified (hypothesis with linear equation), it may be subject to high bias and low variance, making it error-prone. If algorithms fit too complicated a hypothesis (hypothesis with a high degree equation), it may have a large variance and a low bias. In the latter case, the new entries will underperform. There is, however, something in between these two situations called as a Trade-off or\\xa0Bias Variance Trade-off. So, that An algorithm can’t be more complex and less complex at the same time.\\nQ.62 How do we choose the appropriate kernel function in SVM?A kernel function is responsible for converting the original data points into a high dimensionality feature space. Choosing the appropriate kernel function in a Support Vector Machine is a crucial step, as it determines how well the SVM can capture the underlying patterns in your data. Below mentioned are some of the ways to choose the suitable kernel function:\\nIf the dataset exhibits linear relationshipIn this case, we should use Linear Kernel function. It is simple, computationally efficient and less prone to overfitting. For example, text classification, sentiment analysis, etc.\\nIf the dataset requires probabilistic approachThe sigmoid kernel is suitable when the data resembles a sigmoid function or when you have prior knowledge suggesting this shape. For example, Risk assessment, Financial applications, etc.\\nIf the dataset is Simple Non Linear in natureIn this case, use a Polynomial Kernel Function. Polynomial functions are useful when we are trying to capture moderate level of non linearity. For example, Image and Speech Recognition, etc.\\nIf the dataset is Highly Non-Linear in Nature/ we do not know about the underlying relationshipIn that case, a Radial basis function is the best choice. RBF kernel can handle highly complex dataset and is useful when you’re unsure about the data’s underlying distribution. For example, Financial forecasting, bioinformatics, etc.\\nQ.63 How does Naïve Bayes handle categorical and continuous features?Naive Bayes is probabilistic approach which assumes that the features are independent of each other. It calculates probabilities associated with each class label based on the observed frequencies of feature values within each class in the training data. This is done by finding the conditional probability of Feature given a class. (i.e., P(feature | class)). To make predictions on categorical data, Naive Bayes calculates the posterior probability of each class given the observed feature values and selects the class with the highest probability as the predicted class label. This is called as “maximum likelihood” estimation.\\nQ.64 What is Laplace smoothing (add-one smoothing) and why is it used in Naïve Bayes?In Naïve Bayes, the conditional probability of an event given a class label is determined as P(event| class). When using this in a classification problem (let’s say a text classification), there could a word which did not appear in the particular class. In those cases, the probability of feature given a class label will be zero. This could create a big problem when getting predictions out of the training data.\\nTo overcome this problem, we use Laplace smoothing. Laplace smoothing addresses the zero probability problem by adding a small constant (usually 1) to the count of each feature in each class and to the total count of features in each class. Without smoothing, if any feature is missing in a class, the probability of that class given the features becomes zero, making the classifier overly confident and potentially leading to incorrect classifications\\nQ.65 What are imbalanced datasets and how can we handle them?Imbalanced datasets are datasets in which the distribution of class labels (or target values) is heavily skewed, meaning that one class has significantly more instances than any other class. Imbalanced datasets pose challenges because models trained on such data can have a bias toward the majority class, leading to poor performance on the minority class, which is often of greater interest. This will lead to the model not generalizing well on the unseen data.\\nTo handle imbalanced datasets, we can approach the following methods:\\nResampling (Method of either increasing or decreasing the number of samples):Up-sampling: In this case, we can increase the classes for minority by either sampling without replacement or generating synthetic examples. Some of the popular examples are SMOTE (Synthetic Minority Over-sampling Technique), etc.Down-sampling: Another case would be to randomly cut down the majority class such that it is comparable to minority class.Ensemble methods (using models which are capable of handling imbalanced dataset inherently:Bagging\\xa0: Techniques like Random Forests, which can mitigate the impact of class imbalance by constructing multiple decision trees from bootstrapped samplesBoosting: Algorithms like AdaBoost and XGBoost can give more importance to misclassified minority class examples in each iteration, improving their representation in the final modelQ.66 What are outliers in the dataset and how can we detect and remove them?An Outlier is a data point that is significantly different from other data points. Usually, Outliers are present in the extremes of the distribution and stand out as compared to their out data point counterparts.\\nFor detecting Outliers we can use the following approaches:\\nVisual inspection:\\xa0This is the easiest way which involves plotting the data points into scatter plot/box plot, etc.statistics: By using measure of central tendency, we can determine if a data point falls significantly far from its mean, median, etc. making it a potential outlier.Z-score:\\xa0if a data point has very high Z-score, it can be identified as OutlierFor removing the outliers, we can use the following:\\nRemoval of outliers manuallyDoing transformations like applying logarithmic transformation or square rooting the outlierPerforming imputations wherein the outliers are replaced with different values like mean, median, mode, etc.Q.67 What is the curse of dimensionality And How can we overcome this?When dealing with a dataset that has high dimensionality (high number of features), we are often encountered with various issues and problems. Some of the issues faced while dealing with dimensionality dataset are listed below:\\nComputational expense: The biggest problem with handling a dataset with vast number of features is that it takes a long time to process and train the model on it. This can lead to wastage of both time and monetary resources.Data sparsity: Many times data points are far from each other (high sparsity). This makes it harder to find the underlying patterns between features and can be a hinderance in proper analysisVisualising issues and overfitting: It is rather easy to visualize 2d and 3d data. But beyond this order, it is difficult to properly visualize our data. Furthermore, more data features can be correlated and provide misleading information to the model training and cause overfitting.These issues are what are generally termed as “Curse of Dimensionality”.\\nTo overcome this, we can follow different approaches – some of which are mentioned below:\\nFeature Selection: Many a times, not all the features are necessary. It is the user’s job to select out the features that would be necessary in solving a given problem statement.Feature engineering: Sometimes, we may need a feature that is the combination of many other features. This method can, in general, reduces the features count in the dataset.Dimensionality Reduction techniques: These techniques reduce the number of features in a dataset while preserving as much useful information as possible. Some of the famous Dimensionality reduction techniques are: Principle component analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), etc.Regularization:\\xa0Some regularization techniques like L1 and L2 regularizations are useful when deciding the impact each feature has on the model training.Q.68 How does the random forest algorithm handle feature selection?Mentioned below is how Random forest handles feature selection\\nWhen creating individual trees in the Random Forest ensemble, a subset of features is assigned to each tree which is called Feature Bagging. Feature Bagging introduces randomness and diversity among the trees.After the training, the features are assigned a “importance score” based on how well those features performed by reducing the error of the model. Features that consistently contribute to improving the model’s accuracy across multiple trees are deemed more importantThen the features are ranked based on their importance scores. Features with higher importance scores are considered more influential in making predictions.Q.69 What is feature engineering? Explain the different feature engineering methods.Feature Engineering: It can be defined as a method of preprocessing of data for better analysis purpose which involves different steps like selection, transformation, deletion of features to suit our problem at hand. Feature Engineering is a useful tool which can be used for:\\nImproving the model’s performance and Data interpretabilityReduce computational costsInclude hidden patterns for elevated Analysis results.Some of the different methods of doing feature engineering are mentioned below:\\nPrinciple Component Analysis (PCA)\\xa0: It identifies orthogonal axes (principal components) in the data that capture the maximum variance, thereby reducing the data features.Encoding\\xa0– It is a technique of converting the data to be represented a numbers with some meaning behind it. It can be done in two ways :One-Hot Encoding\\xa0– When we need to encode Nominal Categorical DataLabel Encoding\\xa0– When we need to encode Ordinal Categorical DataFeature Transformation: Sometimes, we can create new columns essential for better modelling just by combining or modifying one or more columns.Q.70 How we will deal with the categorical text values in machine learning?Often times, we are encountered with data that has Categorical text values. For example, male/female, first-class/second-class/third-class, etc. These Categorical text values can be divided into two types and based on that we deal with them as follows:\\nIf it is Categorical Nominal Data: If the data does not have any hidden order associated with it (e.g., male/female), we perform One-Hot encoding on the data to convert it into binary sequence of digitsIf it is Categorical Ordinal Data : When there is a pattern associated with the text data, we use Label encoding. In this, the numerical conversion is done based on the order of the text data. (e.g., Elementary/ Middle/ High/ Graduate,etc.)Q.71 What is DBSCAN and How we will use it?Density-Based Spatial Clustering of Applications with Noise (DBSCAN), is a density-based clustering algorithm used for grouping together data points that are close to each other in high-density regions and labeling data points in low-density regions as outliers or noise. Here is how it works:\\nFor each data point in the dataset, DBSCAN calculates the distance between that point and all other data pointsDBSCAN identifies dense regions by connecting core points that are within each other’s predefined threshold (eps) neighborhood.DBSCAN forms clusters by grouping together data points that are density-reachable from one another.Q.72 How does the EM (Expectation-Maximization) algorithm work in clustering?The Expectation-Maximization (EM) algorithm is a probabilistic approach used for clustering data when dealing with mixture models. EM is commonly used when the true cluster assignments are not known and when there is uncertainty about which cluster a data point belongs to. Here is how it works:\\nFirst, the number of clusters K to be formed is specified.Then, for each data point, the likelihood of it belonging to each of the K clusters is calculated. This is called the Expectation (E) stepBased on the previous step, the model parameters are updated. This is called Maximization (M) step.Together it is used to check for convergence by comparing the change in log-likelihood or the parameter values between iterations.If it converges, then we have achieved our purpose. If not, then the E-step and M-step are repeated until we reach convergence.Q.73 Explain the concept of silhouette score in clustering evaluation.Silhouette score is a metric used to evaluate the quality of clusters produced by a clustering algorithm. Here is how it works:\\nthe average distance between the data point and all other data points in the same cluster is first calculated. Let us call this as (a)Then for the same data point, the average distance (b) between the data point and all data points in the nearest neighboring cluster (i.e., the cluster to which it is not assigned)silhouette coefficient for each data point is calculated, which given by: S = (b – a) / max(a, b)if -1<S<0, it signifies that data point is closer to a neighboring cluster than to its own cluster.if S is close to zero, data point is on or very close to the decision boundary between two neighboring clusters.if 0<S<1, data point is well within its own cluster and far from neighboring clusters.Q.74 What is the relationship between eigenvalues and eigenvectors in PCA?In Principal Component Analysis (PCA), eigenvalues and eigenvectors play a crucial role in the transformation of the original data into a new coordinate system. Let us first define the essential terms:\\nEigen Values: Eigenvalues are associated with each eigenvector and represent the magnitude of the variance (spread or extent) of the data along the corresponding eigenvectorEigen Vectors: Eigenvectors are the directions or axes in the original feature space along which the data varies the most or exhibits the most varianceThe relationship between them is given as:\\n[Tex]AV = \\\\lambda{V}\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex], where\\nA = Feature matrix\\nV = eigen vector\\n[Tex]\\\\lambda\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n[/Tex] = Eigen value.\\nA larger eigenvalue implies that the corresponding eigenvector captures more of the variance in the data.The sum of all eigenvalues equals the total variance in the original data. Therefore, the proportion of total variance explained by each principal component can be calculated by dividing its eigenvalue by the sum of all eigenvalues\\nQ.75 What is the cross-validation technique in machine learning?Cross-validation is a resampling technique used in machine learning to assess and validate the performance of a predictive model. It helps in estimating how well a model is likely to perform on unseen data, making it a crucial step in model evaluation and selection. Cross validation is usually helpful when avoiding overfitting the model. Some of the widely known cross validation techniques are:\\nK-Fold Cross-Validation: In this, the data is divided into K subsets, and K iterations of training and testing are performed.Stratified K-Fold Cross-Validation: This technique ensures that each fold has approximately the same proportion of classes as the original dataset (helpful in handling data imbalance)Shuffle-Split Cross-Validation: It randomly shuffles the data and splits it into training and testing sets.Q.76 What are the ROC and AUC, explain its significance in binary classification.Receiver Operating Characteristic (ROC) is a graphical representation of a binary classifier’s performance. It plots the true positive rate (TPR) vs the false positive rate (FPR) at different classification thresholds.\\nTrue positive rate (TPR) : It is the ratio of true positive predictions to the total actual positives.\\nRecall = TP / (TP + FN)\\r\\nFalse positive rate (FPR) : It is the ratio of False positive predictions to the total actual positives.\\nFPR= FP / (TP + FN)\\r\\n\\nArea Under the Curve (AUC) as the name suggests is the area under the ROC curve. The AUC is a scalar value that quantifies the overall performance of a binary classification model and ranges from 0 to 1, where a model with an AUC of 0.5 indicates random guessing, and an AUC of 1 represents a perfect classifier.\\nQ.77 Describe gradient descent and its role in optimizing machine learning models.Gradient descent is a fundamental optimization algorithm used to minimize a cost or loss function in machine learning and deep learning. Its primary role is to iteratively adjust the parameters of a machine learning model to find the values that minimize the cost function, thereby improving the model’s predictive performance. Here’s how Gradient descent help in optimizing Machine learning models:\\nMinimizing Cost functions: The primary goal of gradient descent is to find parameter values that result in the lowest possible loss on the training data.Convergence: The algorithm continues to iterate and update the parameters until it meets a predefined convergence criterion, which can be a maximum number of iterations or achieving a desired level of accuracy.Generalization: Gradient descent ensure that the optimized model generalizes well to new, unseen data.Q.78 Describe batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.Batch Gradient Descent:\\xa0In Batch Gradient Descent, the entire training dataset is used to compute the gradient of the cost function with respect to the model parameters (weights and biases) in each iteration. This means that all training examples are processed before a single parameter update is made. It converges to a more accurate minimum of the cost function but can be slow, especially in a high dimensionality space.\\nStochastic Gradient Descent:\\xa0In Stochastic Gradient Descent, only one randomly selected training example is used to compute the gradient and update the parameters in each iteration. The selection of examples is done independently for each iteration. This is capable of faster updates and can handle large datasets because it processes one example at a time but high variance can cause it to converge slower.\\nMini-Batch Gradient Descent:\\xa0Mini-Batch Gradient Descent strikes a balance between BGD and SGD. It divides the training dataset into small, equally-sized subsets called mini-batches. In each iteration, a mini-batch is randomly sampled, and the gradient is computed based on this mini-batch. It utilizes parallelism well and takes advantage of modern hardware like GPUs but can still exhibits some level of variance in updates compared to Batch Gradient Descent.\\nQ.79 Explain the Apriori — Association Rule MiningAssociation Rule mining is an algorithm to find relation between two or more different objects. Apriori association is one of the most frequently used and most simple association technique. Apriori Association uses prior knowledge of frequent objects properties. It is based on Apriori property which states that:\\n“All non-empty subsets of a frequent itemset must also be frequent”\\nData Science Interview Questions for ExperiencedQ.80 Explain multivariate distribution in data science.A vector with several normally distributed variables is said to have a multivariate normal distribution if any linear combination of the variables likewise has a normal distribution. The multivariate normal distribution is used to approximatively represent the features of specific characteristics in machine learning, but it is also important in extending the central limit theorem to several variables.\\nQ.81 Describe the concept of conditional probability density function (PDF).In probability theory and statistics, the conditional probability density function (PDF) is a notion that represents the probability distribution of a random variable within a certain condition or constraint. It measures the probability of a random variable having a given set of values given a set of circumstances or events.\\nQ.82 What is the cumulative distribution function (CDF), and how is it related to PDF?The probability that a continuous random variable will take on particular values within a range is described by the Probability Density Function (PDF), whereas the Cumulative Distribution Function (CDF) provides the cumulative probability that the random variable will fall below a given value. Both of these concepts are used in probability theory and statistics to describe and analyse probability distributions. The PDF is the CDF’s derivative, and they are related by integration and differentiation.\\nQ.83 What is ANOVA? What are the different ways to perform ANOVA tests?The statistical method known as ANOVA, or Analysis of Variance, is used to examine the variation in a dataset and determine whether there are statistically significant variations between group averages. When comparing the means of several groups or treatments to find out if there are any notable differences, this method is frequently used.\\nThere are several different ways to perform ANOVA tests, each suited for different types of experimental designs and data structures:\\nOne-Way ANOVATwo-Way ANOVAThree-Way ANOVAWhen conducting ANOVA tests we typically calculate an F-statistic and compare it to a critical value or use it to calculate a p-value.\\nQ.84 How can you prevent gradient descent from getting stuck in local minima?Ans:\\xa0The local minima problem occurs when the optimization algorithm converges a solution that is minimum within a small neighbourhood of the current point but may not be the global minimum for the objective function.\\nTo mitigate local minimal problems, we can use the following technique:\\nUse initialization techniques like Xavier/Glorot and He to model trainable parameters. This will help to set appropriate initial weights for the optimization process.Set Adam or RMSProp as optimizer, these adaptive learning rate algorithms can adapt the learning rates for individual parameters based on historical gradients.Introduce stochasticity in the optimization process using mini-batches, which can help the optimizer to escape local minima by adding noise to the gradient estimates.Adding more layers or neurons can create a more complex loss landscape with fewer local minima.Hyperparameter tuning using random search cv and grid search cv helps to explore the parameter space more thoroughly suggesting right hyperparameters for training and reducing the risk of getting stuck in local minima.Q.85 Explain the Gradient Boosting algorithms in machine learning.Gradient boosting techniques like XGBoost, and CatBoost are used for regression and classification problems. It is a boosting algorithm that combines the predictions of weak learners to create a strong model. The key steps involved in gradient boosting are:\\nInitialize the model with weak learners, such as a decision tree.Calculate the difference between the target value and predicted value made by the current model.Add a new weak learner to calculate residuals and capture the errors made by the current ensemble.Update the model by adding fraction of the new weak learner’s predictions. This updating process can be controlled by learning rate.Repeat the process from step 2 to 4, with each iteration focusing on correcting the errors made by the previous model.Q.86\\xa0Explain convolutions operations of CNN architecture?In a CNN architecture, convolution operations involve applying small filters (also called kernels) to input data to extract features. These filters slide over the input image covering one small part of the input at a time, computing dot products at each position creating a feature map. This operation captures the similarity between the filter’s pattern and the local features in the input. Strides determine how much the filter moves between positions. The resulting feature maps capture patterns, such as edges, textures, or shapes, and are essential for image recognition tasks. Convolution operations help reduce the spatial dimensions of the data and make the network translation-invariant, allowing it to recognize features in different parts of an image. Pooling layers are often used after convolutions to further reduce dimensions and retain important information.\\nQ.87\\xa0What is feed forward network and how it is different from recurrent neural network?Deep learning designs that are basic are feedforward neural networks and recurrent neural networks. They are both employed for different tasks, but their structure and how they handle sequential data differ.\\nFeed Forward Neural Network\\nIn FFNN, the information flows in one direction, from input to output, with no loopsIt consists of multiple layers of neurons, typically organized into an input layer, one or more hidden layers, and an output layer.Each neuron in a layer is connected to every neuron in the subsequent layer through weighted connections.FNNs are primarily used for tasks such as classification and regression, where they take a fixed-size input and produce a corresponding outputRecurrent Neural Network\\nA recurrent neural network is designed to handle sequential data, where the order of input elements matters. Unlike FNNs, RNNs have connections that loop back on themselves, allowing them to maintain a hidden state that carries information from previous time steps.This hidden state enables RNNs to capture temporal dependencies and context in sequential data, making them well-suited for tasks like natural language processing, time series analysis, and sequence generation.However, standard RNNs have limitations in capturing long-range dependencies due to the vanishing gradient problem.Q.88 Explain the difference between generative and discriminative models?Generative models focus on generating new data samples, while discriminative models concentrate on classification and prediction tasks based on input data.\\nGenerative Models:\\nObjective: Model the joint probability distribution P(X, Y) of input X and target Y.Use: Generate new data, often for tasks like image and text generation.Examples: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs).Discriminative Models:\\nObjective: Model the conditional probability distribution P(Y | X) of target Y given input X.Use: Classify or make predictions based on input data.Examples: Logistic Regression, Support Vector Machines, Convolutional Neural Networks (CNNs) for image classification.Q.89 What is the forward and backward propogations in deep learning?Forward and backward propagations are key processes that occur during neural network training in deep learning. They are essential for optimizing network parameters and learning meaningful representations from input.\\nThe process by which input data is passed through the neural network to generate predictions or outputs is known as forward propagation. The procedure begins at the input layer, where data is fed into the network. Each neuron in a layer calculates the weighted total of its inputs, applies an activation function, and sends the result to the next layer. This process continues through the hidden layers until the final output layer produces predictions or scores for the given input data.\\nThe technique of computing gradients of the loss function with regard to the network’s parameters is known as backward propagation. It is utilized to adjust the neural network parameters during training using optimization methods such as gradient descent.\\nThe process starts with the computation of the loss, which measures the difference between the network’s predictions and the actual target values. Gradients are then computed by using the chain rule of calculus to propagate this loss backward through the network. This entails figuring out how much each parameter contributed to the error. The computed gradients are used to adjust the network’s weights and biases, reducing the error in subsequent forward passes.\\nQ.90 Describe the use of Markov models in sequential data analysis?Markov models are effective methods for capturing and modeling dependencies between successive data points or states in a sequence. They are especially useful when the current condition is dependent on earlier states. The Markov property, which asserts that the future state or observation depends on the current state and is independent of all prior states. There are two types of Markov models used in sequential data analysis:\\nMarkov chains are the simplest form of Markov models, consisting of a set of states and transition probabilities between these states. Each state represents a possible condition or observation, and the transition probabilities describe the likelihood of moving from one state to another.Hidden Markov Models extend the concept of Markov chains by introducing a hidden layer of states and observable emissions associated with each hidden state. The true state of the system (hidden state) is not directly observable, but the emissions are observable.Applications:\\nHMMs are used to model phonemes and words in speech recognition systems, allowing for accurate transcription of spoken languageHMMs are applied in genomics for gene prediction and sequence alignment tasks. They can identify genes within DNA sequences and align sequences for evolutionary analysis.Markov models are used in modeling financial time series data, such as stock prices, to capture the dependencies between consecutive observations and make predictions.Q.91 What is generative AI?Generative AI is an abbreviation for Generative Artificial Intelligence, which refers to a class of artificial intelligence systems and algorithms that are designed to generate new, unique data or material that is comparable to, or indistinguishable from, human-created data. It is a subset of artificial intelligence that focuses on the creative component of AI, allowing machines to develop innovative outputs such as writing, graphics, audio, and more. There are several generative AI models and methodologies, each adapted to different sorts of data and applications such as:\\nGenerative AI models such as GPT (Generative Pretrained Transformer) can generate human-like text.” Natural language synthesis, automated content production, and chatbot responses are all common uses for these models.Images are generated using generative adversarial networks (GANs).” GANs are made up of a generator network that generates images and a discriminator network that determines the authenticity of the generated images. Because of the struggle between the generator and discriminator, high-quality, realistic images are produced.Generative AI can also create audio content, such as speech synthesis and music composition.” Audio content is generated using models such as WaveGAN and Magenta.Q.92 What are different neural network architecture used to generate artificial data in deep learning?Various neural networks are used to generate artificial data. Here are some of the neural network architectures used for generating artificial data:\\nGANs consist of two components – generator and discriminator, which are trained simultaneously through adversarial training. They are used to generating high-quality images, such as photorealistic faces, artwork, and even entire scenes.VAEs are generative models that learn a probabilistic mapping from the data space to a latent space. They also consist of encoder and decoder. They are used for generating images, reconstructing missing parts of images, and generating new data samples. They are also applied in generating text and audio.RNNs are a class of neural networks with recurrent connections that can generate sequences of data. They are often used for sequence-to-sequence tasks. They are used in text generation, speech synthesis, music composition.Transformers are a type of neural network architecture that has gained popularity for sequence-to-sequence tasks. They use self-attention mechanisms to capture dependencies between different positions in the input data. They are used in natural language processing tasks like machine translation, text summarization, and language generation.Autoencoders are neural networks that are trained to reconstruct their input data. Variants like denoising autoencoders and contractive autoencoders can be used for data generation. They are used for image denoising, data inpainting, and generating new data samples.Q.93 What is deep reinforcement learning technique?Deep Reinforcement Learning (DRL) is a cutting-edge machine learning technique that combines the principles of reinforcement learning with the capability of deep neural networks. Its ability to enable machines to learn difficult tasks independently by interacting with their environments, similar to how people learn via trial and error, has garnered significant attention.\\nDRL is made up of three fundamental components:\\nThe agent interacts with the environment and takes decision.The environment is the outside world with which the agent interacts and receives feedback.The reward signal is a scalar value provided by the environment after each action, guiding the agent toward maximizing cumulative rewards over time.Applications:\\nIn robotics, DRL is used to control robots, manipulation and navigation.DRL plays a role in self-driving cars and vehicle controlCan also be used for customized recommendationsQ.94 What is transfer learning, and how is it applied in deep learning?Transfer learning is a strong machine learning and deep learning technique that allows models to apply knowledge obtained from one task or domain to a new, but related. It is motivated by the notion that what we learn in one setting can be applied to a new, but comparable, challenge.\\nBenefits of Transfer Learning:\\nWe may utilize knowledge from a large dataset by starting with a pretrained model, making it easier to adapt to a new task with\\xa0data.Training a deep neural network from scratch can be time-consuming and costly in terms of compute. Transfer learning enables us to bypass the earliest phases of training, saving both time and resources.Pretrained models frequently learn rich data representations. Models that use these representations can generalize better, even when the target task has a smaller dataset.Transfer Learning Process:\\nFeature ExtractionIt’s a foundation step in transfer learning. The pretrained data is already trained on large and diverse dataset for a related task.To leverage the knowlege, output layers of the pretrained model are removed leaving the layers responsible for feature extraction. The target data is passed through these layers to extract feature information.using these extracted features, the model captures patterns and representations from the data.Fine TuningAfter the feature extraction process, the model is fine-tuned for the specific target task.Output layers are added to the model and these layer are designed to produce the desired output for the target task.Backpropagation is used to iteratively update the model’s weights during fine-tuning. This method allows the model to tailor its representations and decision boundaries to the specifics of the target task.Even as the model focuses in the target task, the knowledge and features learned from the pre-trained layers continue to contribute to its understanding. This dual learning process improves the model’s performance and enables it to thrive in tasks that require little data or resources.Q.95 What is difference between object detections and image segmentations.Object detection and Image\\xa0segmentation are both computer vision tasks that entail evaluating and comprehending image content, but they serve different functions and give different sorts of information.\\nObject Detection:\\ngoal of object detection is to identify and locate objects and represent the object in bounding boxes with their respective labels.used in applications like autonomous driving for detecting pedestrians and vehicleImage Segmentation:\\nfocuses on partitioning an image into multiple regions, where each segment corresponding to a coherent part of the image.provide pixel level labeling of the entire imageused in applications that require pixel level understanding such as medical image analysis for organ and tumor delineation.Q.96 Explain the concept of word embeddings in natural language processing (NLP).In NLP, the concept of word embedding is use to capture semantic and contextual information. Word embeddings are dense representations of words or phrases in continuous-valued vectors in a high-dimensional space. Each word is mapped to a vector with the real numbers, these vectors are learned from large corpora of text data.\\nWord embeddings are based on the Distributional Hypothesis, which suggests that words that appear in similar context\\xa0have similar meanings. This idea is used by word embedding models to generate vector representations that reflect the semantic links between words depending on how frequently they co-occur with other words in the\\xa0text.\\nThe most common word embeddings techniques are-\\nBag of Words (BOW)Word2VecGlove: Global Vector for word representationTerm frequency-inverse document frequency (TF-IDF)BERTQ.97 What is seq2seq model?A neural network architecture called a Sequence-to-Sequence (Seq2Seq) model is made to cope with data sequences, making it particularly helpful for jobs involving variable-length input and output sequences. Machine translation, text summarization, question answering, and other tasks all benefit from its extensive use in natural language processing.\\nThe Seq2Seq consists of two main components: encoder and decoder. The encoder takes input sequence and converts into fixed length vector . The vector captures features and context of the sequence. The decoder takes the vector as input and generated output sequence. This autoregressive technique frequently entails influencing the subsequent prediction using the preceding one.\\nQ.98\\xa0What is artificial neural networks.Artificial neural networks take inspiration from structure and functioning of human brain. The computational units in ANN are called neurons and these neurons are responsible to process and pass the information to the next layer.\\nANN has three main components:\\nInput Layer: where the network receives input features.Hidden Layer:\\xa0one or more layers of interconnected neurons responsible for learning patterns in the dataOutput Layer: provides final output on processed information.Q.99 What is marginal probability?A key idea in statistics and probability theory is marginal probability, which is also known as marginal distribution. With reference to a certain variable of interest, it is the likelihood that an event will occur, without taking into account the results of other variables. Basically, it treats the other variables as if they were “marginal” or irrelevant and concentrates on one.\\nMarginal probabilities are essential in many statistical analyses, including estimating anticipated values, computing conditional probabilities, and drawing conclusions about certain variables of interest while taking other variables’ influences into account.\\nQ.100 What are the probability axioms?The fundamental rules that control the behaviour and characteristics of probabilities in probability theory and statistics are referred to as the probability axioms, sometimes known as the probability laws or probability principles.\\nThere are three fundamental axioms of probability:\\nNon-Negativity AxiomNormalization AxiomAdditivity AxiomConclusionWe all know that data science is growing career and if you are looking a future in data science, then explore this detailed article on data science interview questions. \\n \\n\\n\\nLike Article\\n\\n\\n\\n\\n\\n\\n\\nSuggest improvement\\n\\n\\n\\n  \\n\\nNext\\n\\n\\n\\n\\nData Science Interview Questions and Answers\\n\\n\\n\\n\\n\\nShare your thoughts in the comments\\n\\n\\nAdd Your Comment\\n\\n \\n\\n\\n\\n\\n\\n  Please Login to comment...\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/'})]\n"
     ]
    }
   ],
   "source": [
    "#No.2.1 web loader Specific class wise loader more ifficient\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_path=(\"https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/\",)\n",
    "                     ,bs_kwargs=dict(parse_only= bs4.SoupStrainer(class_=(\"article-title\",\"text\"))))\n",
    "data=loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No 3 pdf loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"reserchpaper.pdf\")\n",
    "pdfload= loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text splitting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Page 1 of 3 \\n \\n \\n \\n \\n  \\nTejas Ahirrao  \\n3rd Year Student  \\nDept.of COMP  \\nSKN Sinhgad Institute of  \\nTechnology and Science, Lonavala, Pune. \\ntejasahirrao.sknsits@sinhgad.edu  \\n \\nTejashree Khaire  \\n3rd Year Student  \\nDept.of COMP  \\nSKN Sinhgad Institute of  \\nTechnology and Science, Lonavala, Pune.  \\ntejashreekhaire.sknsits.comp@gmail.com  \\n \\nAbstract—In the busy metropolitan cities like Mumbai, Delhi , \\npeople don't have time to invest in waiting for transport. Waiting\", metadata={'source': 'reserchpaper.pdf', 'page': 0}), Document(page_content=\"people don't have time to invest in waiting for transport. Waiting \\ntime for transport in such crowded cities leads to less productivity \\non a whole. People face this problem in their daily life where the y \\nhave no idea about the current status of their transport. So the \\nproposed solution is an android based application that will help \\nthe user to check out the current location of the bus and also will \\nhelp the user to know how much time the bus will  take to reach the\", metadata={'source': 'reserchpaper.pdf', 'page': 0}), Document(page_content='help the user to know how much time the bus will  take to reach the \\ncurrent location of the user. The system will use GPS as the basis \\nfor the application and basic android application will be \\ninterfacing with the updated database to provide the real -time \\ndata to the user, hence enhancing the user -experi ence.  \\n \\nKeywords — GPS Module, Android Studio.  \\n \\nI. INTRODUCTION   \\nThere are buses available for passengers travelling to \\ndifferent locations, but not many passengers have complete', metadata={'source': 'reserchpaper.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)\n",
    "splitted=textsplitter.split_documents(pdfload)\n",
    "print(splitted[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector Databse indexing, embbeding and retriver object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 35/35 [01:14<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splitted, embedding= OllamaEmbeddings(model=\"nomic-embed-text\",show_progress= True))\n",
    "\n",
    "retriver = vectorstore.as_retriever() #can use multipul type of serach like MMR or similarity(defult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chain for the rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# prompt\n",
    "prmpt= PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of\n",
    "                                         retrieved context to answer the question. If you don't know the answer, just say that \n",
    "                                         you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \"\"\")\n",
    "\n",
    "#llm \n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# output parser \n",
    "outputparser = StrOutputParser()\n",
    "\n",
    "#chain\n",
    "rag_chain=(\n",
    "    {\"context\": retriver , \"question\": RunnablePassthrough()}\n",
    "    |prmpt\n",
    "    |llm\n",
    "    |outputparser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This document is about a research paper that proposes a \"Real Time Bus Tracking System\" which allows users to track the location of buses in real-time.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is this document about\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
